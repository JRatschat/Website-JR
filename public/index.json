[{"authors":["admin"],"categories":null,"content":"I am passionate about data. Why? Because data can uncover hidden structures and enable informed decisions. For me, this is an art. Instead of using a brush and paint, I use various programming languages and econometric or machine learning methods to create analyses that generate fascinating insights.\n I want to share this fascination with others.\n At TechAcademy, a student initiative at Goethe University, we prepare students for the digital future. We are convinced that everyone can benefit from knowing how to code. That\u0026rsquo;s why we do our best to pass on this knowledge through online courses, group projects, and inspiring events.\nI also like to share my projects and insights from time to time. Therefore I have set up this website to keep you up to date üöÄ\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"I am passionate about data. Why? Because data can uncover hidden structures and enable informed decisions. For me, this is an art. Instead of using a brush and paint, I use various programming languages and econometric or machine learning methods to create analyses that generate fascinating insights.","tags":null,"title":"","type":"authors"},{"authors":["Âê≥ÊÅ©ÈÅî"],"categories":null,"content":"Âê≥ÊÅ©ÈÅî is a professor of artificial intelligence at the Stanford AI Lab. His research interests include distributed robotics, mobile computing and programmable matter. He leads the Robotic Neurobiology group, which develops self-reconfiguring robots, systems of self-organizing robots, and mobile sensor networks.\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Sed neque elit, tristique placerat feugiat ac, facilisis vitae arcu. Proin eget egestas augue. Praesent ut sem nec arcu pellentesque aliquet. Duis dapibus diam vel metus tempus vulputate.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"d8f258c323db746988131e8c2d192f9a","permalink":"/author/%E5%90%B3%E6%81%A9%E9%81%94/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/%E5%90%B3%E6%81%A9%E9%81%94/","section":"authors","summary":"Âê≥ÊÅ©ÈÅî is a professor of artificial intelligence at the Stanford AI Lab. His research interests include distributed robotics, mobile computing and programmable matter. He leads the Robotic Neurobiology group, which develops self-reconfiguring robots, systems of self-organizing robots, and mobile sensor networks.","tags":null,"title":"Âê≥ÊÅ©ÈÅî","type":"authors"},{"authors":null,"categories":null,"content":"This article was first published on Medium.\nI wanted to determine if news providers‚Äô ideology can be objectively classified by performing some common natural language processing tasks on news articles.\nBut let‚Äôs start from the beginning‚Ä¶\nPolitical division is a vital problem in the U.S. Nowadays, even what we eat, what we drive, and how we live is associated with either being Democrat or Republican. Decisions like wearing or not wearing a mask become part of political identity.\nThis political division‚Äôs foundation was laid when people with college degrees moved to the cities, and less-educated people stayed in rural areas in the 1970s. Jobs, technologies, and industries followed accordingly, and the cultural differences between Democrats and Republicans were amplified with the emergence of partisan media and social media networks [1].\nIf you think about it: People are exposed to different kinds of information. What people read and what people hear plays a vital role in how they see the world.\nSo if people are exposed to a news provider that is considered as ‚Äúfar-right‚Äù and ‚Äúideologically driven,‚Äù for example, Breitbart News [2], the world view of this news provider could have a notable influence on their reader‚Äôs world view.\nWould I recognize the political ideology of a news provider by just reading some of their articles? And how about other people? Imagine reading ‚Äúfar-right‚Äù articles and thinking that this is considered ‚Äúnormal‚Äù‚Ä¶\n This is why I started to think about how I can objectively classify news providers‚Äô ideology.\n Ultimately, I had the idea that Breitbart News would use, for example, more negative words when reporting about Joe Biden while using more positive comments for Trump than other news outlets.\nHence, articles written about a topic that agrees with a particular ideology would have a positive sentiment, while disagreeing issues would negatively affect sentiment.\nUsing data from four news providers and applying sentiment and topic models, I will test if my hypotheses are true. If you‚Äôre interested in knowing how I conducted the analysis using Python and R, please look at my GitHub repository. There you can find everything that you need.\nData I focused on articles from Breitbart News, Politico, Axios, and FiveThirtyEight for the analysis. Let‚Äôs first understand who these news providers are and what they stand for.\nI have already introduced Breitbart News, but I will use their wording this time. They stand for ‚Äútruthful reporting and the free and open exchange of ideas‚Äù because this is ‚Äúessential to maintaining a robust democracy.‚Äù Also, they believe ‚Äúin the greatness of America.‚Äù\nAxios is a news provider that oughts to ‚Äúdeliver the clearest, smartest, most efficient, and trustworthy experience for audience and advertisers alike.‚Äù They claim not to follow any ideology. Also, their style of writing was described as a mix of Twitter and the Economist.\nPolitico‚Äôs main topics are politics and policy, and their goal is to become the dominant news source on these topics. The stand for ‚Äúaccess to reliable information, nonpartisan journalism, and real-time tools‚Äù to ‚Äúcreate, inform and engage a global citizenry.‚Äù\nLast, FiveThirtyEight follows a data-driven coverage of politics and sports and uses common data visualization and statistical analyses to provide a unique news experience.\nI have to admit: This is a wild mix. Not the typical news providers that you would find in a research paper. So why have I chosen these news providers?\n  First, I needed some news sources that have a different ideology than Breitbart News. Axios explicitly states that it does not follow any ideology. Therefore, it should be neutral when it comes to ideology. The same goes for Politico that stands for nonpartisan news coverage. For FiveThirtyEight, it‚Äôs not as clear. I assumed a more left-leaning position due to their reports in the 2016 election that argued that ‚ÄúDonald Trump isn‚Äôt a real candidate‚Äù and gave him minimal chances of winning the election.\n  Second, I would have loved to gather articles from more well-known left-leaning news providers like the New York Times or Washington Post. Sadly, the providers that I had a look at often had websites based on Javascript (instead of HTML), making it harder to extract information with the approach that I used. Also, many news providers have a paywall making it impossible to extract articles without paying for it.\n  After I had decided which news providers I wanted to analyze, I gathered articles on the political segments of these news providers. Also, I only extracted pieces from 2020 to have a somehow limited amount of topics.\nThese decisions led to a data set that consists of approximately 32.5k articles. Breitbart News contributes around 20k articles to the data set, while I only extracted around 500 items from FiveThirtyEight. Also, there are roughly 7.8k articles from Politico and 3.7k from Axios. Therefore, the findings for Axios, Breitbart News, and Politico should be more robust than the results for FiveThirtyEight.\nI found it interesting to explore some of the differences between the news providers. You can see in the plot below that Axios publishes the shortest articles (it‚Äôs no surprise that they are compared to Twitter), closely followed by Breitbart News. Politico and FiveThirtyEight tend to have longer articles.\n\r\r\r\rSentiment Now that I had extracted the data that I needed, it was time to calculate every article‚Äôs sentiment. I decided to use the well-established VADER model [3], which is predominantly used for social media postings. However, it was also tested with New York Times Editorials and achieved the best F1 score compared to other sentiment analysis lexicons. Hence, it can also be applied to other areas.\nThe model uses a generalizable sentiment lexicon. The list was inspired by the renowned word-banks LIWC, ANEW, and G.I., but the researchers also included emoticons and slang into the dictionary. Interestingly, it not only relies on a fixed sentiment score for each word but also incorporates grammatical and syntactic cues. To take some examples from their paper:\n  Booster words: ‚ÄúThe service here is extremely good‚Äù has a stronger positive sentiment than ‚ÄúThe service is good.‚Äù\n  Contrastive conjunction: ‚ÄúThe food here is great, but the service is horrible.‚Äù VADER would sense a mixed sentiment with the latter part of the sentence being dominant.\n  Shifts in polarity: ‚ÄúThe food here isn‚Äôt really all that great.‚Äù Although this sentence includes the positive word great, VADER can classify the negative sentiment.\n  Although VADER was not built with newspaper articles in mind, I finally decided that its theoretical foundation would be a ‚Äúgood-enough‚Äù fit for my analysis.\nAdditionally, VADER becomes handy when working with more massive data sets because it is computationally cheap and does not require training data. Although other algorithms like Google‚Äôs BERT could have led to more accurate results, I still remembered the endless computing times that I had with another project, and I was not willing to go through this again.\nWhen plotting the mean sentiment of the articles by week, two striking insights became apparent to me:\n  There are differences in the sentiment scores between the news providers. FiveThirtyEight tends to have the most positive articles on average, while Breitbart News has the most negative articles on average compared to the other news providers.\n  The news provider‚Äôs sentiment develops alike when looking at it over time. For example, all news providers see a strong negative drop of the sentiment score in week 23 and 24 with a recovery in the following weeks. This finding indicates that some topics or occurrences are classified as more positive or negative by VADER than other issues. For example, a major adverse event could have happened in week 23 because there is such a drop in sentiment scores.\n  \r\r\r\rTopics Since different events are very likely to affect the news articles‚Äô sentiment, it becomes crucial to classify the items into various topics. If not, the sentiment scores would be influenced by the topics, and ideology classification would not be feasible.\nLatent Dirichlet Allocation (LDA) is an unsupervised machine learning model that extracts the main topics of a given set of words. It assumes that every document consists of a mix of different topics. By going through the documents, LDA extrapolates which topics could have generated them.\nI used LDA to generate topics and to calculate the topic probabilities for each article. For the analysis, I only stored the subject with the highest topic probability. Ultimately, I ended up with 20 overarching topics for January to August 2020. Instead of looking at every topic in detail (which would be boring for you), let‚Äôs look at the most prominent issues.\nYou can see the number of articles per day in the below plot, depending on the topic. Some of the issues seem of relatively low importance, while other topics show a high presence.\n\r\r\r\rFrom first sight, Topic 6, 10, and 18 do look the most interesting. I created word clouds and looked at articles with the highest topic probabilities to better understand these topics.\nBy looking at the word cloud for Topic 6 and the articles with the highest topic contributions, it is relatively sure that these articles are about the Democratic presidential candidates.\n\r Topic 6\r\r\rTopic Contribution: 100%\r\u0026quot;Why Bernie Sanders Lost\u0026quot; (FiveThirtyEight)\rTopic Contribution: 99%\r\u0026quot;Electile Dysfunction: Joe Biden Pulls Out Early from New Hampshire\u0026quot; (Breitbart News)\rTopic Contribution: 99%\r\u0026quot;Joe Biden says Iowa caucus results were a \u0026quot;gut punch\u0026quot;\u0026quot; (Axios)  This overarching topic also makes sense when having a look at article count over time. Until March, this topic was of high interest to news providers and the public. Then the interest decreased after Buttigieg, Bloomberg, and Warren dropped out of the Democratic race. Interest increased shortly when Sanders suspended his campaign on April 8 and stayed low until Biden officially won the nomination on August 18.\nSo how about Topic 10?\n\r Topic 10\r\r\rYou guessed right: Topic 10 mostly consists of articles related to the coronavirus. The word cloud and the article count per day makes sense. I additionally checked the articles with the highest topic contributions:\nTopic Contribution: 99%\r\u0026quot;China Has Not Accepted U.S. Offers to Send CDC Experts to Aid in Coronavirus Fight\u0026quot; (Breitbart News)\rTopic Contribution: 99%\r\u0026quot;How many Americans have been tested for coronavirus?\u0026quot; (Politico)\rTopic Contribution: 99%\r\u0026quot;Azar: Coronavirus 'a fast-moving, constantly changing situation\u0026quot; (Politico)\r Topic 18 seemed relatively easy to spot when only looking at the word cloud and the time trend. For me, it was screaming ‚ÄúBlack Lives Matter‚Äù!\n\r Topic 18\r\r\rBut when I was assessing the articles with the highest topic contributions, I was confused first. Generally, LDA gave news articles with police operations the highest topic contributions.\nTopic Contribution: 99%\r\u0026quot;Armed Homeowner Kills One Alleged Intruder, Wounds Second\u0026quot; (Breitbart News)\rTopic Contribution: 97%\r\u0026quot;Pennsylvania Homeowner Shoots Alleged Intruder Dead\u0026quot; (Breitbart News)\rTopic Contribution: 96%\r\u0026quot;Watch: Daytona Beach Police Shoot Alleged Armed Carjacker\u0026quot; (Breitbart News)\r But this would have not explained why there was this upheaval in articles during the riots. When looking at articles with lower topic contribution scores, the articles that I had expected appeared.\nTopic Contribution: 90%\r\u0026quot;Philadelphia Cops Injured with Chemical Burns During Violent Protests\u0026quot; (Breitbart News)\rTopic Contribution: 90%\rTitle: \u0026quot;Protesters Set NYPD Cars Ablaze in Union Square\u0026quot; (Breitbart News)\rTopic Contribution: 88%\rTitle: \u0026quot;Police Cars in LA, Chicago Damaged and Destroyed by Protesters\u0026quot; (Breitbart News)\r This topic intrigued me. Most articles of Breitbart concerning this topic were in favor of the police. When I was looking at reports from Axios, this view diametrically changed to more centered items on what had happened to protesters. This is a significant difference in ideology, but it‚Äôs questionable if this difference can be tracked by news sentiment because both points of view should be rather harmful.\nMultiple Linear Regression After I generated variables for both sentiment and topic analyses, I was finally able to conduct a regression estimation. First, I ran a multiple linear regression to see how the topics and news providers (independent variables) are related to the news article‚Äôs sentiment (dependent variable). I used robust standard errors since the errors are heteroskedastic.\nSentiment·µ¢ = Œ≤‚ÇÄ + Œ≤‚ÇÅ‚®ØDominant_Topic·µ¢ + Œ≤‚ÇÇ‚®ØNewsProvider·µ¢ + ùúÄ·µ¢\n\rAxios together with Topic 0 is the base case and hidden in the intercept‚Äôs coefficient. Topic 5 (e.g., attack, terrorist, hong kong, iran) and 18 (e.g., police, riots, black) is associated with more negative sentiment while topic 6 (e.g., biden, democrat, campaign) is associated with a more positive effect on sentiment. Also, Politico and FiveThirtyEight tend to report more positively than Axios (as indicated in the intercept) and Breitbart News.\nBut we have to be careful here because the p-value of Breitbart News is nowhere near the commonly used significance levels. Therefore, we can‚Äôt reject the Null hypothesis that there is no difference in conditional means between Breitbart News and its articles‚Äô sentiment.\nAlthough this regression enables some first insights, it is not enough to understand the underlying ideology of a news provider. Remember: I hypothesized that news providers would have differing sentiments about different topics.\nMultiple Linear Regression with Interaction Effects Therefore, I decided to conduct a multiple linear regression with interaction effects. By using interaction effects, it is now feasible to analyze the combined effects of the dominant topics and news providers. Also, I still used robust standard errors in the estimation.\n Sentiment·µ¢ = Œ≤‚ÇÄ + Œ≤‚ÇÅ‚®ØDominant_Topic·µ¢ + Œ≤‚ÇÇ‚®ØNewsProvider·µ¢ + Œ≤‚ÇÉ‚®Ø(Dominant_Topic·µ¢‚®ØNewsProvider·µ¢) + ùúÄ \nSome of the p-values of the topics and also Breitbart News and Politico are not near the commonly used significance levels.\nStill, in the plot below you can see the predicted sentiment per topic and news provider. When searching for differences in sentiment and checking the p-values for each interaction effect in the regression table, only Topic 6 seems interesting. Remember: this topic is predominantly about the Democratic presidential candidates.\n\r\r\r\r**Predicted Sentiment per News Provider for Topic 6**\rAxios: 0.54\rBreitbart News: 0.44\rFiveThirtyEight: 0.75\rPolitico: 0.66\r Breitbart did indeed have the articles with the lowest sentiment. When taking a close look at the interaction terms for Topic 6, it becomes apparent that the coefficient of Breitbart News is the only one with a negative association. Still, I have to admit that I was somehow disappointed. Only one out of 20 topics showed a result that somehow followed my hypothesis. Also, the differences between the predicted sentiment scores seemed not as high as I would have expected.\nAssessment So what did I do? And how did it go? What did I learn?\nFirst, I gathered data from four different news providers. Pretty amazing what one can do with a web crawler like scrapy. I was incredibly happy that I was able to gather the articles from the news provider‚Äôs websites. A downer was, however, that I could not scrape articles from the New York Times or the Washington Post. These news providers would have better reflected the Democratic element for my analysis.\nSecond, calculating the sentiment with the VADER module and looking at it over time led to interesting insights. I was able to spot that the news provider‚Äôs sentiment develops alike over time. This finding led me to the hypothesis that topics or occurrences are inherently either positive or negative. Big up- or downswings can happen when major events occur.\nThird, classifying news articles into topics using LDA went smoothly. I was impressed by how topics like the ‚ÄúDemocratic presidential nomination‚Äù or the ‚ÄúCoronavirus‚Äù were accurately classified and how much sense the article count over time made.\nExploring the topics led to realizing the first notable difference between Breitbart News and the other news providers: One can influence an event‚Äôs perspective. Breitbart News focused predominantly on what did happen to police officers during protests and riots while the others focused more on the protesters. This difference alone is a strong indicator of a news provider‚Äôs ideology.\nThird, conducting the regression estimations presented answers to some of the hypotheses that I had made. Yes, sentiment scores are very different, depending on the topic. Also, the estimation did shed light on the general tendency of a news provider‚Äôs sentiment. But this alone was of no help in classifying a news provider‚Äôs ideology. Only the interaction effects of Topic 6 were promising, but sadly this topic was the only one where my hypothesis somehow made sense.\nConclusion Ultimately, I think that my approach to classifying ideology by comparing sentiment scores failed. I still like the idea, but it seems that topics that are perceived as positive or negative depending on the political view are relatively rare.\nBut I found one very interesting insight during the topic analysis: News providers reported quite differently about the Black Lives Matters protests. Sadly (for my analysis), the sentiment was not affected by this difference.\nStill, I have done some groundwork for further investigations. If I have some more time and you are interested in it, I could imagine myself going back to the Black Lives Matter topic. Finding some methods that can objectively show the differences that I indicated would be awesome.\nIf you have any questions or comments, feel free to reach me via the contact field or LinkedIn.\nStay tuned, and see you in the next post!\nReferences [1] https://edition.cnn.com/interactive/2019/11/opinions/fractured-states-of-america/part-one-fredrick/\n[2] https://en.wikipedia.org/wiki/Breitbart_News\n[3] Hutto, C.J. \u0026amp; Gilbert, E.E. (2014). VADER: A Parsimonious Rule-based Model for Sentiment Analysis of Social Media Text. Eighth International Conference on Weblogs and Social Media (ICWSM-14). Ann Arbor, MI, June 2014.\n","date":1601078400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1601078400,"objectID":"58b4b24f23f9e746909a303d4bb10174","permalink":"/post/ideology-classification-of-u.s.-news-through-sentiment/","publishdate":"2020-09-26T00:00:00Z","relpermalink":"/post/ideology-classification-of-u.s.-news-through-sentiment/","section":"post","summary":"Not as easy as I thought.","tags":["Data Science"],"title":"Ideology Classification of U.S. News through Sentiment","type":"post"},{"authors":null,"categories":["Data Science"],"content":"","date":1601078400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1601078400,"objectID":"2eadb9a9861b08b2b34194cddedd51a2","permalink":"/project/us-news/","publishdate":"2020-09-26T00:00:00Z","relpermalink":"/project/us-news/","section":"project","summary":"I gathered U.S. news articles through web scraping and conducted sentiment, topic, and regression analyses in this repository.","tags":["Data Science","Research"],"title":"U.S. News","type":"project"},{"authors":null,"categories":null,"content":"This article was first published on Medium.\nPete Buttigieg, former mayor of South Bend, Indiana, gained US-wide popularity when running as a presidential candidate for the 2020 election.\nHe was able to secure robust fund-raising and achieved surprisingly good results in the states of Iowa and New Hampshire. After not being able to hold up his momentum, he dropped out of the Democratic race. Still, I was intrigued after first hearing about his story.\nBeing born in the Rust Belt, he earned degrees from Harvard and Oxford, worked as a consultant for McKinsey, served in the Afghanistan War, and became mayor of South Bend at the age of 29. Although facing controversies during his two terms as a mayor, he has convinced residents and stakeholders that his hometown is more than just a former manufacturing location.\nI picked up his book Shortest Way Home and was surprised to find out that he is a firm believer in data-driven decision making. Instead of making decisions based on gut-feeling, he argues that analytical analysis helps the city become more efficient and fact-driven.\nComing from consulting, he first assumed that more data and analytics would always be beneficial for his administration. But soon, he realized that many resources could be wasted due to data generated either not being used or only giving information that is already known.\n He had to make sure that data was not only generated but that it also served a purpose.\n Pete Buttigieg has come up with six crucial learnings from his terms as a mayor that I want to share with you.\n\r Photo by Fred Moon on Unsplash \r\r\r1. There is a difference between reporting and solving an issue. Sometimes reporting and solving an issue can go hand in hand.\nFor example, Pete Buttigieg‚Äôs administration installed ShotSpotter technology that uses microphones to spot gunshots in the whole city. On the one hand, the administration got reliable data on where shots were fired. This was essential because some neighborhoods did not call the police when gunshots were fired. On the other hand, police officers could get much faster to cases of gun violence.\nBut often, knowing more about the problem does not help in solving it.\nFor instance, the former mayor attended a conference in which a startup introduced a product that could automatically detect opioids‚Äô patterns in sewage. The administration was well aware of the problem of opioid usage in South Bend but lacked funds to take care of the mental health and addiction resources. In acquiring the opioid detecting product, the administration would have gathered more data on this problem. But it would also have taken away valuable resources from initiatives solving this issue.\nTherefore, he argues in his book that when one is already aware of a problem and has the means to fix it, one should focus on resolving the issue. This does not mean that reporting an issue is unnecessary, but it is also not sufficient to solve the problem.\n2. There is a difference between responsiveness and efficiency. During Pete Buttigieg‚Äôs term as a mayor, he had to decide various times between being responsive or efficient.\nIn the case of snowplowing, he could have decided that the snowplowing crews had always been driven to the streets first for whom residents called in. This would have made these streets residents indeed very happy, but this had not been an efficient approach.\nWhy? Because these crews would zigzag through the city. By using a zone-based approach, snowplow crews can take care of all streets much faster. But this means that not every impassable road is directly taken care of.\nAt other times, Pete Buttigieg argues that being responsive is the most efficient thing. For instance, graffiti should be taken care of as soon as it is reported because other people could be motivated to imitate or surpass this graffiti. Also, if it is directly taken care of, graffiti sprayers are discouraged from defacing public property.\n\r Photo by Robert V. Ruggiero on Unsplash \r\r\r3. Be honest whether you are willing to follow the data where it leads. For this learning, the former mayor had to decide on how he could keep waste bill rates under control.\nHe asked one of his public work directors for help. He got presented various ideas: selling ad space on trash bins, charging customers based on the amount of waste, or introducing automated trash trucks that use a robotic arm.\nThe automated trash trucks were the best solution, partly eliminating human garbage collectors and reducing injury rates. The data was clear that this new technology would lead to lower waste bill rates.\n This is a tough decision. Replacing humans with machines often means that some people will lose their jobs.\n He decided to go forward with this proposal, and half of the workers laid off were able to switch jobs to other employments offered by the city.\nAt other times, Pete Buttigieg was not ready to follow the data.\nOffices, where people can go to pay their water bills, seem to be of another time. People can pay online, by phone or email. So why is it still necessary to have this costly infrastructure? Because especially residents with low income do not have bank accounts. They need a place where they can pay in cash even if it is costly for the city.\nSure, one could follow the data on this case, but this would mean severe harm to these low-income residents. So are you really willing to follow the data where it leads?\n4. Data can show you answers to questions you never even asked. Sometimes, searching for an answer can lead to an answer to another question.\nRemember the example with the gunshot technology? Gathering this data and getting faster help for gunshot cases helped in answering another crucial question. To which extent do neighborhoods trust the police? The fewer neighbors called (if any), the lower would be the perceived police legitimacy.\nAlso, this enabled officers to show up even if nobody bothered to call. Neighbors were expecting that the police would not care about gunshots in their area. By coming to these neighborhoods, the police were able to increase trust.\nTherefore, it can be helpful to ask yourself the question: ‚ÄúIs there another issue that I can solve with the data?‚Äù\n\r Photo by timJ on Unsplash \r\r\r5. Confusion between technical and moral questions. Answering technical questions seem so easy because often they are right or wrong questions. But when it comes to moral issues, it is often impossible to make someone better off without making another one worse off.\nFor example, the city decided to get these new automatic trash trucks. Although this decreased waste bills rates, it led to layoffs. Instead of keeping the trash bins in the alleys, residents were now pressured to haul their trash bins to the front curb. Of course, the workers and residents were not happy with this change.\nWhat Pete Buttigieg stated for this decision is that there was no mathematical formula to solve this trade-off. Therefore, they had to make the decision and explain to those affected why they took this decision.\n6. Exceptions are important. Data-driven decision making is centered around rules. And if a rule is not sufficient, a sub-rule can be implemented. But sometimes that is not enough. Instead of following the rules, a better direction is to make an exception even though one cannot explain or defend this exception.\nSo, for example, an older man called the city to ask for help with a dead raccoon. Following the rule book would have had a clear answer: The city has no obligation to help the man because the raccoon is on the man\u0026rsquo;s private yard.\nBut obviously, the man was calling because he could not help himself. Instead of following the rule book, a council member drove to the man‚Äôs house, dragged the raccoon onto the street, and made sure that the case was handled.\nConclusion Ultimately, Pete Buttigieg is a believer in data-driven decision making. He believes that it can enable people to make smarter and fairer decisions. And yet he is well aware that data has limitations and that not every problem can be solved by it. Also, he is determined that there are decisions in which one must follow his or her intuitions.\nI enjoyed reading his book for several reasons. Although I was excited about his chapter on data-driven decisions (Chapter 11-Subconscious Operations), I also learned much more about American politics and the Rust Belt‚Äôs struggles.\nIf you have any questions or comments, feel free to reach me via the contact field or LinkedIn.\nStay tuned, and see you in the next post!\nReferences [1] Pete Buttigieg (2019), Shortest Way Home: One Mayor‚Äôs Challenge and a Model for America‚Äôs Future, Liveright, pp. 388‚Äì418.\n","date":1597622400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1597622400,"objectID":"ae1289cb89432bd6fa29a9eadf078ad9","permalink":"/post/what-pete-buttigieg-has-to-say-about-data-driven-decision-making/","publishdate":"2020-08-17T00:00:00Z","relpermalink":"/post/what-pete-buttigieg-has-to-say-about-data-driven-decision-making/","section":"post","summary":"Former democratic presidential candidate and ‚ÄúTech-Mayor‚Äù firmly believes in data and analysis.","tags":["Data Science"],"title":"What Pete Buttigieg Has to Say About Data-Driven Decision Making","type":"post"},{"authors":null,"categories":null,"content":"This article was first published on Medium.\nAccuracy and interpretability are said to be diametrically different. Complex models tend to achieve the highest accuracies, while simpler models tend to be more interpretable.\nBut what if we want to get the best of both worlds? Getting a high accuracy is cool, but wouldn‚Äôt it be great if we can understand the model? Especially, if the prediction is essential to the success of a business? As Ribeiro et al. state:\n ‚Äú[‚Ä¶] if the users do not trust a model or a prediction, they will not use it.‚Äù (Ribeiro et al. 2016, p. 1)\n Hypothetical Situation Let‚Äôs imagine the following: We work as data scientists at a huge savings bank. Our job is to make sure that we know which customers have a high likelihood to churn.\nWhat will happen with our model? We will use our results to create customer groups with differing churn probabilities. The customer groups with the highest churn probabilities then receive targeted marketing so that they hopefully do not leave the bank.\nRetaining customers is associated with much lower costs compared to acquiring new customers (Keaveney 1995). Therefore, our job is crucial for the success of the bank!\nWe must achieve two goals:\n  The marketing team must receive the most accurate results possible. Why? Because the bank is spending money to retain these customers. If the customers would have stayed anyway, the bank is throwing money out of the window.\n  We must convince relevant stakeholders that they can trust our results. Good luck explaining to the marketing boss how we created our model‚Ä¶\n  So let‚Äôs imagine that the marketing boss does not understand our model, but trusts us (for whatever reason). Therefore, they implement our complex model.\n This model has high predictive power. The marketing team is happy because they are seeing that the measures conducted help to retain customers. We are so glad because we get positive feedback for the model.  Nice! We could call it a day and focus our efforts on the next task to even receive more credits for the great work that we do.\nBUT STOP! HOLD ON!\nAccuracy and interpretability cannot be achieved with the same model, right?\nTherefore, we focused only on the model‚Äôs accuracy. But what if the model has flaws?\nWe could have included one variable in our model that has literally no meaning. Somehow, it worked for a while, but then it didn‚Äôt, and nobody recognized it.\nEven worse: We could have created a sexist algorithm that causes an uproar on social media. Do you think I‚Äôm kidding? This has somehow happened to Apple and Goldman Sachs.\n Would a manager risk his/her career for only a few accuracy points more?\n I can give you a plain and simple answer here‚Ä¶ No, managers won‚Äôt take the risk. They have spent many years building their careers, and they don‚Äôt know anything about machine learning.\nOf course, predicting churn probabilities is probably not a hot-topic that will lead to an uproar in the media. But if our model is not working in our favor because we failed to understand it correctly, the consequences can be painful.\nThe Solution Interpretability is essential because it promotes confidence and trust in our model. Users do not adopt a model that fails to do so. Also, enhanced interpretability can help to improve model performance and extends the knowledge that we can derive from the model (Lundberg \u0026amp; Lee 2017).\nNice! By increasing interpretability, we are not only able to understand our model, but we could gain valuable insights. Maybe the marketing team would love to get more ideas about their customers?\nLuckily, eliminating the trade-off between a model‚Äôs accuracy and a model‚Äôs interpretability has gained attention by researchers (Ribeiro et al. 2016, Lundberg \u0026amp; Lee 2017, Chen et al. 2018).\n One of their solutions has gained a lot of attention: SHapley Additive exPlanations (SHAP) values introduced by Lundberg \u0026amp; Lee (2017).\n SHAP values stem from game theory. They measure the marginal effect that the observed level of a variable has on the final predicted probability for an individual (Lundberg et al. 2019).\nThrough this approach, it becomes feasible to explain why a customer receives its churn prediction value and how the features contribute to this prediction. This local interpretability increases transparency. Also, it becomes feasible to combine local explanations to derive global interpretability.\nLet‚Äôs use these mysterious SHAP values to transform a complex model into a complex AND interpretable model.\nAnd Action Remember, we want to know which customers will churn. Therefore, I use this Kaggle data set for predicting churn probabilities of bank customers.\n\rFor the prediction, I used a basic logistic regression and a default XGBoost algorithm. To make the post somehow readable, I won‚Äôt show lines of code. If you‚Äôre interested in knowing how I build the model, please have a look at my GitHub repository. There you can find everything that you need.\nSo let‚Äôs quickly compare model statistics of the logit model and the default XGBoost model.\nThe logit model reaches a test accuracy of 84.1 percent while the default XGBoost model has a test accuracy of 86.6 percent. How about AUC? Here, the logit model has an AUC of 82.3 percent compared to an AUC of 85.1 percent for the default XGBoost model. Indeed, the more complex model is better in predicting who‚Äôs going to churn and who‚Äôs going to stay at the bank.\nSo what we could generally use for XGBoost are three different importance measures through which we can have a more detailed look on our model.\n\rImportance plots of XGBoost model. Plot created by author.\r\r\rWhat comes to my mind? The three importance measures assign different levels of importance to the features.\nAlso, I cannot see the relationship between the independent variables and churn. Does a higher age increase or decrease the probability of being a churner? The plots do not help answer this question.\nSo let‚Äôs have a look at how our model is interpreted using SHAP values.\nIn the figure below, features are ordered by their global impact on the default XGBoost model. Higher SHAP values represent a higher prediction of a customer exiting the bank. The dots represent the feature impact on the model output of individual predictions of the training set. Depending on the feature value, the dots are colored from blue (low) to red (high). Therefore, we can explore the extent, strength, and direction of a feature‚Äôs effect.\n\rSHAP summary plot. Plot created by author with [SHAPforxgboost](https://github.com/liuyanguu/SHAPforxgboost).\r\r\rThe highest global feature importances are taken by Age, NumOfProducts, and IsActiveMember. There are several interesting insights that we can derive from this summary plot:\n  The range of feature impacts on the forecast is widely distributed. For example, age has a strong effect on the prediction for some customers, while for others, it only has a small effect.\n  Although some of the variables have low global importance, the feature‚Äôs influence can be powerful for some individuals. For example, the credit score has a positive predictive relationship with churn for people with low credit scores.\n  At first sight, some variables show an intuitive relation to churn, while for others, it is counterintuitive. For example, why is a higher number of products related to an increase in the churn probability? Does this make sense?\n  To explore single variables more closely, we can use dependence plots. In the figure below, a SHAP dependence plot for age is displayed. As with the summary plot, each dot is a customer. As shown, younger customers have a lowered predictive value to churn, while older customers have an increased predictive value to churn. But this is not a linear relationship! When reaching the age of 60, customer churn probabilities start to decrease.\n\rSHAP dependence plot. Plot created by author with [SHAPforxgboost](https://github.com/liuyanguu/SHAPforxgboost).\r\r\rGood luck exploring this relationship with a logit model‚Ä¶\n\rSummary output of logistic regression based on the same data as the XGBoost model. Age variable is highlighted.\r\r\rTo get crazy, we can also look at the interaction effects of variables.\nOn the left panel below, you can see the same dependence plot for the age variable. But this time, I added the gender variable to see how age and gender interact with each other.\nOn the right panel, the interaction effect of age and gender is depicted more closely. Somehow (and I am lacking the domain knowledge to interpret this), there is a different behavior of men and women when paired with age. For example, younger women have negative SHAP interaction value meaning that the SHAP value is lower for young women compared to young men.\n\rSHAP dependence plot (left panel) and SHAP interaction plot (right panel) for age and gender. Plots created by author with [SHAPforxgboost](https://github.com/liuyanguu/SHAPforxgboost).\r\r\rSo far, so good. But how about the mentioned effect of the number of products that the customers have on the churn prediction? At least for me, it seems counterintuitive that customers with more products have a higher likelihood of churning. We all know how hard it is to terminate contracts, especially bank contracts.\nWhen looking at the coefficient values of the logistic regression, we see the same effect. Somehow, customers that have two products have a lower churn probability compared to customers that have one, three, or four products.\n\rSummary output of logit model based on the same data as the XGBoost model. NumOfProducts2 variable is highlighted.\r\r\rTherefore, I assume that this trend is present in the data and not randomly used by our XGBoost model.\nAm I still worried? Yes. What would I usually do?\n  Look at the literature. This relationship could make sense, who knows?\n  Question the relevance of the data. I already had the topic in mind when I was searching for a data set. I did not have a lot of alternatives, and this data set has no copyrights. The algorithm is only as good as the data.\n  Explore the data set. An explanatory analysis before building the model is highly advisable. Maybe, I could find some patterns that can explain this counterintuitive result.\n  Delete the variable. If I suspected that the variable does not provide meaningful results and could lead to flaws, I would delete it. Even if that would mean losing some accuracy or AUC percentage points.\n  The analysis that I have conducted so far could be done for every variable. We need to fully understand our model, right? But since this article has gotten really long (thumps up if you made it so far!), I will spare you further analysis.\nConclusion It is vital that we understand our predictive model and that this model captures the generalized underlying trends of the data. If not, the company faces a ticking time bomb. All could go well until it doesn‚Äôt anymore.\nAlso, we need to able to explain our results to relevant stakeholders. Explaining predictive models and first foremost SHAP values to someone who has trouble understanding basic statistics is hard! Still, these stakeholders are often the decision-makers, so we must make sure that we can convince them that our model really works in practice.\nBesides mitigating possible risks by increasing the trust in our model, we can get additional insights that we wouldn‚Äôt get with simpler models. This can be highly relevant. Especially for marketing departments who are always interested in understanding their customers.\nIf you have any questions or comments, feel free to reach me via the contact field or LinkedIn.\nStay tuned, and see you in the next post!\nReferences [1] Ribeiro, M. T., Singh, S. \u0026amp; Guestrin, C. (2016), ‚ÄúWhy should I trust you?‚Äù: Explaining the predictions of any classifier, in B. Krishnapuram \u0026amp; M. Shah, eds, ‚ÄòKDD ‚Äô16: Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining‚Äô, pp. 1135‚Äì1144.\n[2] Keaveney, S. M. (1995), ‚ÄòCustomer switching behavior in service industries: An exploratory study‚Äô, Journal of Marketing 59, 71‚Äì82.\n[3] Lundberg, S. \u0026amp; Lee, S.-I. (2017), A unified approach to interpreting model predictions, in U. von Luxburg, I. M. Guyon, S. Bengio, H. M. Wallach \u0026amp; R. Fergus, eds, ‚ÄòNIPS‚Äô17: Proceedings of the 31st International Conference on Neural Information Processing Systems‚Äô, pp. 4768‚Äì4777.\n[4] Chen, J., Le Song, Wainwright, M. J. \u0026amp; Jordan, M. I. (2018), Learning to explain: An information-theoretic perspective on model interpretation, in J. G. Dy \u0026amp; A. Krause, eds, ‚ÄòProceedings of the 35th International Conference on Machine Learning‚Äô, pp. 882‚Äì891.\n[5] Lundberg, S. M., Erion, G. G. \u0026amp; Lee, S.-I. (2019), ‚ÄòConsistent individualized feature attribution for tree ensembles‚Äô, pp. 1‚Äì9.\n","date":1594512000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1594512000,"objectID":"f37d2f082e87669eac473c8c7e688833","permalink":"/post/how-to-increase-the-interpretability-of-your-predictive-model/","publishdate":"2020-07-12T00:00:00Z","relpermalink":"/post/how-to-increase-the-interpretability-of-your-predictive-model/","section":"post","summary":"Practical insights from a business perspective.","tags":["Data Science"],"title":"How to Increase the Interpretability of Your Predictive Model","type":"post"},{"authors":null,"categories":null,"content":"This article was first published on Medium.\nTwo years ago, I was taking a beginner‚Äôs class in R at my university. I had heard of the term data science, but I didn‚Äôt even know that this would be my first exposure to this exciting world.\nHonestly, I was just taking this class to meet people. The week quickly passed, I learned some of the basics and got interested in programming. But I didn‚Äôt know how I should proceed.\nSo I did some additional online classes on DataCamp but quickly stopped. Why? Even though the website itself is an excellent starting point, I just did not have goals that would keep me going.\n My life went on, I had other responsibilities, and coding was not a priority anymore.\n Fast forward two years. I have conducted several data science projects ranging from prediction to text mining tasks in Python and R. More importantly, my team and I at TechAcademy have supported 190 students in their journey to learn to code. Students with different backgrounds and experiences.\nI want to share my perspective with you. It‚Äôs a personal assessment of what works best to get results fast. Keep in mind that everyone is different. So if you\n are just starting to learn to code, can‚Äôt stand to do another online class or get relatively fast bored with online assignments, or wonder which philosophy we used to teach 190 students coding,  then keep on reading.\nSo what changed? How could I go from zero to where I am now?\nUltimately, the only thing that counts is motivation. I had to make sure that coding became a priority. Up until then, my thoughts around coding were based on extrinsic motivation. This is what was going on in my head:\n Oh yeah, I heard that coding is essential in the 21st century. I will have much better opportunities in the job market if I know how to handle some lines of code.\n I had to switch it to a more intrinsically motivated way of thinking.\n I really like coding. It‚Äôs fun to get exciting results with every line of code that I write. It enables me to do really crazy shit.\n Who do you think has more stamina? The oh-I‚Äôm-having-better-job-opportunities-guy or the person that really just loves coding for the sake of coding?\nBut to get to this state, you have to put in the work. You can‚Äôt love coding when you have only watched a few tutorials. Coding is not a quick game of Rock Paper Scissors. Coding is more like a long game of Risk.\nIn the beginning, it‚Äôs hard to understand the rules. But once you‚Äôre in the flow, every new situation gets easier. This is the point that you want to reach. You want to understand the underlying rules of the programming language of your choice. Afterward, every new project or method is doable and exciting.\nBut how can you reach that state? And how can we make sure that you are going to love coding?\nBasics Online classes have their place, especially for beginners. They are a perfect starting place for taking on the basics.\nSo if you‚Äôve never written a line of code, head over to an online learning platform like Datacamp (affiliate link) or Udemy. There you can start to understand the programming language that you want to learn.\nBut how many online classes should you do? I recommend sticking to the basics. Try to keep it simple. Why? Because after some classes you should have quite a good idea of how this programming language works.\nYou could go on and learn everything there is through online classes. I doubt that this is the most efficient and effective way of learning data science.\nSome classes teach you every way possible to load data into R or Python. dta-, csv-, excel-, json-, whatever-files‚Ä¶ But do you really need to spend 6 hours of your valuable time to learn this? It‚Äôs way more efficient to just google how to load your specific file when you actually have a file that you want to use. This should be sufficient, right?\nFor me, getting an overload of information that I‚Äôm not going to use is pointless. It‚Äôs a waste of time. Also, I‚Äôm just not good at following online classes. I get bored very fast. And this kills my motivation. If you enjoy online courses, then you can, of course, take more classes. But please be intentional about them.\nTaking it to the next level Now that you have learned the basics, how should you proceed? How can we make sure that you‚Äôre staying motivated?\nThe answer is plain and simple. You have to create something on your own. Take the things that you have learned and create your first personal project. Learning by doing. You need to realize what power you have achieved by just learning the basics.\n But Jonathan, I really don‚Äôt know what kind of project I should do. Can you help me out?\n Sure, I got your back!\nAt TechAcademy, we give our students projects in which they get the data and have to perform explanatory analyses. With this simple task, they have to load the data,\n\rhave a first look on it,\n\rbring it into the right format (data manipulation),\n\rand get first insights by creating plots.\n\r\rPlot based on Lukas J√ºrgernsmeier‚Äôs TechAcademy Solution\r\r\rAlthough that‚Äôs no rocket science, this is an integral part of being a data scientist. To work with the data, you have to understand the data. This is only a simple example. You could go crazy.\nSo why don‚Äôt you go on Kaggle right now and load the data from the famous Titanic data set? If you are a beginner, don‚Äôt try to predict something right away. Just load the data, look at the structure, and try to find some insights. This helps you to apply your new skills. This is not enough? Go on to the tidytuesday repository on Github and measure your data tidying and visualization skills with the community.\nFrom there on, you can add to your skillset with every project that you conduct. If explanatory analyses get boring, try to learn what machine learning is. Read some articles about it and get started with predicting the survival variable of the Titanic data set. If you‚Äôve mastered this, how about doing some text mining stuff?\nAt least for me, tackling projects is so much fun. I am using the tools that I learned to explore or predict stuff. How cool is that compared to an online class where you just do what you‚Äôre told to? Additionally, you‚Äôre building a portfolio that you could share with your community. This is more exciting than presenting the 1000th certificate of an online class.\nFollowing step-by-step tutorials with final tests or little projects are okay. But creating a project of your own requires much more thinking and creativity. You really have to understand the underlying data and use it to reach your goals. This is a critical skill of a data scientist.\nOf course, it‚Äôs not always easy. You‚Äôre getting errors, and you don‚Äôt know why. Every line of code could become a disaster. But it is how it is. Most likely, this error has not only happened to you. Stackoverflow or just any other side is your savior. Just type the error message in Google search, and you‚Äôll be helped.\nConclusion Learning to code to become a data scientist doesn‚Äôt have to be an elusive rabbit that you cannot catch.\nThere are manageable and actionable strategies you can employ to reach your goals very quickly with a lot of fun.\nThe strategies are as follows:\n1) Stay patient and let your motivation grow. Make sure you understand why you want to learn to code. Generally, every one of us lives in a digital world that is exponentially changing. Having coding skills (not limited to data science) is a great skill that can help you to reach your goals. But be sure that you find ways to enjoy it. Give it some time. Data science becomes great after you have gathered the necessary skills.\n2) Generally, only use online classes in the beginning. Online courses are a great way to learn the basics. But there is no need to learn everything through online classes. Also, they can get boring, at least for me. Make sure that you don‚Äôt pressure yourself to complete all the classes of a data science track if you don‚Äôt enjoy this kind of learning. Being a data scientist is not defined as someone who is having a certificate laying around somewhere.\n3) Take your skills to the next level by conducting projects. Projects are fun. They enable you to get creative with the skills that you have learned. If you want to learn something new, read some articles and try it out. Take one project at a time, make it harder, and reap the benefits.\nStarting to code is not an easy task. I have seen many people struggle with the first few steps. Once you‚Äôve kept showing up, everything becomes more accessible. I hope that these strategies help you to enjoy coding and especially data science.\nIf you have any questions or comments, feel free to reach me via the contact field or LinkedIn.\nStay tuned, and see you in the next post!\n","date":1592697600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1592697600,"objectID":"69a217d1c783588ee8fde7c3e67a95e9","permalink":"/post/how-190-students-and-i-have-learned-data-science/","publishdate":"2020-06-21T00:00:00Z","relpermalink":"/post/how-190-students-and-i-have-learned-data-science/","section":"post","summary":"Actionable strategies on how to learn to code.","tags":["Data Science"],"title":"How 190 Students and I Have Learned Data Science","type":"post"},{"authors":null,"categories":null,"content":"I was failing. I had worked my ass off. But somehow, I did not see any success.\nIt was not that I wasn\u0026rsquo;t reaching anyone. I got meetings with interested potential clients. But after having had numerous pitches, I did not see any results.\nI got rejected. Often. I heard all the excuses.\n Sorry, we don\u0026rsquo;t have have any budget right now. Let\u0026rsquo;s talk in half a year. I talked with the relevant division. They don\u0026rsquo;t have time to organize something. Your proposal sounds interesting. I\u0026rsquo;ll forward you to another part of the company where you\u0026rsquo;ll fit better.  And of course, I never heard back from the company.\nAt that time, I decided to work harder. To approach more clients. To get more meetings. Somewhere I had heard that hard work would ultimately pay off as a sales guy. Success was waiting for me.\nBut of course, nothing changed. The system that I had built was running, but not in my favor.\nOne day I met with one of the founders for lunch. All of a sudden, he said to me, \u0026ldquo;Jonathan, it\u0026rsquo;s not working, right?\u0026rdquo;. I responded, \u0026ldquo;I\u0026rsquo;m doing everything I can, but I don\u0026rsquo;t know what\u0026rsquo;s going on\u0026rdquo;.\n What he then told me was one of the best advice I\u0026rsquo;d ever been given.\n \u0026ldquo;Jonathan, I can\u0026rsquo;t help you. But I bet there is someone who can help you. Why don\u0026rsquo;t you ask Justin? He is one of the best sales guys I know\u0026rdquo;.\nI was on fire. Justin, yeah, I knew this guy. Startup founder. Cool guy, approachable, and lots of experience.\nOverly motivated as I was, I not only reached out to him but also his sales head. One thing led to another, and I had two meetings scheduled.\nWhat happened? We talked a lot about my sales process. About every little detail that I had never thought of. After meeting both of them for 30 minutes each, I was able to point out the mistakes I had made. Not just the mistakes, but also the things I was not doing.\nWhat happened then? I still put in the hard work. But this time, everything changed. Instead of zero offers, I had three on the table. Since we only had one spot open, I had to cancel two potential partners.\n When looking back, these mistakes seem so obvious. But little did I know back then.\n I had not asked anyone for advice when I started my new role. Everyone seemed to be determined that I was the very best candidate for this role. Therefore, it never crossed my mind to ask for help.\nAlso, when I was not successful, I did not question my approach. I mean, I was approaching clients and meeting with them. It was just bad luck that I did not get the response that I wanted.\nIt took someone else to open my eyes. And when my eyes were open, I was able to seek the advice I needed. And when I sought the direction I needed, my actions changed for good.\nThis is how I seek advice. I just ask for it. When I have a problem that Google cannot solve, I think about who I can ask. Do I know anyone who could get me going in the right direction? If not, do I have a friend that knows a lot of people in this particular field?\nWhen I have identified someone that can genuinely help me, I sent a message. Straightforward. Where? Most often, they are acquaintances, so I either have their phone number, or I am connected on LinkedIn with them.\nI state my problem. What is important here is that I am specific about my problem. That I cannot solve it by myself. Also, I show respect by asking this particular person for advice. I trust this person to be able to help me.\nWhat I ask for is a phone call. Most often, they are glad to help. I mean, who doesn\u0026rsquo;t like to help someone who is actively seeking one\u0026rsquo;s advice? Someone who shows respect and trust?\nI know it takes courage to ask an acquaintance or even a stranger for help. But ultimately, their input can help me to solve problems that I could not have solved by myself.\nWhat I\u0026rsquo;ve learned through this experience is that hard work is not always the solution. That I\u0026rsquo;m not an expert just because my job description says so. Therefore, I am allowed to get advice. And this advice can help me to get to the very next level.\nIf you have any questions or comments, feel free to reach me via the contact field or LinkedIn.\n","date":1591488000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1591488000,"objectID":"2ae4db4d8d50b23621ee100e60824812","permalink":"/post/an-expert-is-made-not-born/","publishdate":"2020-06-07T00:00:00Z","relpermalink":"/post/an-expert-is-made-not-born/","section":"post","summary":"Asking for advice is easy. It helped me when working harder was not the answer.","tags":["Personal Insights","Student Initiative","Sales"],"title":"An Expert is Made, Not Born","type":"post"},{"authors":null,"categories":null,"content":"This article was first published on Medium.\nHow great would it be when data could be as easily accessed as on Kaggle?\nData collection? Just visit Kaggle, find a suitable data set, and download it. How about analyzing the Titanic incident from 1912? Or how about image recognition of flowers? No? Maybe you want to predict credit card fraud detection? Guess what, Kaggle has you covered.\nWhen you have decided on your data set of interest, the fun can finally start. Taking 5 minutes to find a suitable data set was already stressful enough, right? The most elaborate machine algorithms are waiting for you. So ultimately, who cares about the data?\nSadly, the real world is different. Having the right data and data quality are key to making causal statements or to constructing machine learning algorithms that can really have an impact. Without relevant data, your analyses would be fun, but irrelevant.\nObviously, you cannot always find perfectly preprocessed data that fulfill your needs. Also, you need to understand where the data has come from and how it was built. Ultimately, we need to keep in mind what Kaggle is.\n It is most famous for being a place in which one can enter competitions to solve data science challenges.\n So if you want to make a name for yourself as a serious predictive modeler, you have found the perfect website to show off your skills. If you‚Äôre going to gather data to write a research paper or to build something that works in the real world, Kaggle could be the wrong source for your data.\nIn my experience, data collection and preparation can take days to complete. What I have done so far is to build data sets from scratch and access data sets from government institutions. Both have their limitations.\nWhat I want to show you is a practical introduction to how you could create relevant data sets that support you in your research/machine learning goals. Let‚Äôs get started.\nFirst, you have to assess the following two questions to conduct your analysis.\n What kind of data do you need? How can you access it?  Answering these questions is critical but not always straightforward. Of course, a Google search could lead to results, but asking peers for advice could also be helpful. Spend some time with these questions until you‚Äôre sure that you have found the right answer.\n1. Building a data set from scratch In one of my projects, I needed to access financial data from German companies to analyze the effect of a new mandatory accounting standard on bid-ask spreads.\nLuckily, my professor supplied us with a Thomson Reuters account, and I could use Datastream to access the financial data of these companies. You would think that simply using this database would be sufficient and that I could finally do the real work.\nFalse! When gathering the data for these companies, I ended up with 8 different excel sheets that I had to somehow merge into one data frame.\nDatastream provided me with some static company information that would end up as my main sheet.\n\rThe other excel sheets that I got had the following format because I was accessing time-series data for each company.\n\rSo how can I get such data into a meaningful format so that I can use it along with the other company information?\nLet‚Äôs perform one of my calculations so that you get the idea. I had two sheets ‚Äî one for bid prices and one for ask prices. What I needed was the average relative bid-ask spread.\nFirst, I loaded the data and controlled for missing values. I spotted one row that was completely missing and deleted it for both data sets.\n\rThen I calculated the bid-ask spread by subtracting the bid price from the ask price.\n\r'data.frame': 152 obs. of 50 variables:\r$ D.AB1 : num 0.034 0.069 0.038 ...\r$ D.AOX : num 0.38 0.36 0.38 ...\r$ D.AAD : num 0.38 0.4 0.36 ...\r$ D.CAG : num 0.04 0.1 0.04 ...\r$ D.B5A : num 0.36 0.395 0.395 ...\r$ D.BDT : num 0.37 0.75 1 ...\r$ D.BIO3: num 0.84 0.82 0.82 ...\r$ D.O2C : num 0.151 0.15 0.15 ...\r$ D.CEV : num 0.305 0.295 0.2 ...\r$ D.CWC : num 0.535 1.175 1.335 ...\r Then I had to calculate the relative bid-ask spread. Therefore, I had to import the daily stock prices, deleted the 149th row, and calculated the bid-ask spreads relative to the price.\n\rFinally, I calculated the mean of the relative bid-ask spreads and merged it into the static data frame.\n\r'data.frame': 50 obs. of 6 variables:\r$ MNEM : chr \u0026quot;D.2HRA\u0026quot; \u0026quot;D.AAD\u0026quot; \u0026quot;D.AB1\u0026quot; ...\r$ NAME : chr \u0026quot;H \u0026amp; R\u0026quot; \u0026quot;AMADEUS FIRE\u0026quot; ...\r$ WC05350 : POSIXct, format: \u0026quot;2011-12-31\u0026quot; ...\r$ MV : num 644 150 331 638 622 ...\r$ NOSHFF : num 44 74 63 44 52 45 100 ...\r$ mean_relative_bid_ask: num 0.0138 0.0139 0.0163 ...\r This is only the code for one additional variable! Imagine doing that for 20 or even 30 other variables that you cannot get out-of-the-box from Datastream. This takes way longer than 5 minutes.\nThere are many other feasible methods of how you can create your own data set from scratch. You could, for example, conduct a good old survey or scrape tweets from Twitter. Ultimately, it depends on what kind of data you need.\nOkay, it‚Äôs time for a quick assessment.\nAdvantages:\n Features are included based on the purpose of the research question or task. Not vice versa. This helps to only use meaningful data. It is traceable how the variables were created.  Disadvantages:\n It can be challenging to find suitable sources. It takes a lot of time to gather the data. Transforming features into the right format can be a lot of effort. Access to databases like Thomson Reuters is often restricted. If your university or employer does not have a license, this kind of information can get very costly.  2. Using a data set from a governmental institution One could think that accessing data from governmental institutions is as easy as obtaining data from Kaggle. Wrong! Often, you have to put in a lot of time to understand the data.\nSo for another project, I wanted to research the effect of financial literacy on stock market participation. For assessing this research question, I found quite a lot of research papers that made use of the Survey of Consumer Finances to analyze stock market participation. Therefore, I accessed the newest version (2016) of this cross-sectional survey of US families.\n Little did I know how difficult understanding and working with this data set would be.\n So the SEC has published all relevant data here. I spend a substantial time finding the right data set. First, I tried out the R-implementation, but in the end, I found it too difficult to use. Then, I accidentally downloaded the summary extract and wondered why the data was so different than described in the codebook. After a long journey of losing my mind, I found the complete data that I would be using for my analyses.\nWhen looking at the data, I found that all variables were encoded. I had to use this codebook to make sense of the data. For every single variable‚Ä¶\n\rThe codebook is really really long. Even the SEC notes on the first lines of the codebook the crazy size of this document. It contains about 45,000 lines of text. They recommend to not print the entire document. Great advice‚Ä¶\n\rFirst lines of the SCF‚Äôs Codebook.\r\r\rI spend many hours finding the questions that I could use to create meaningful variables. Ever used the search tool in Chrome? I probably used it 10,000 times for finding the right variables.\nLuckily, the summary extract that I first downloaded proofed to be helpful. Instead of calculating the numerous financial information of the households in R, I just merged this data set to the complete survey data set. Funny story: The calculations are specified in a SAS script. I attempted to translate it into R. After wasting two hours, I remembered the summary extract‚Ä¶\nFinally, I had a data set that I could start to analyze. But of course, I had many more problems with it. Have you ever heard of weighting and multiple imputations? These topics are difficult and painful, at least for me. But this is another story to tell.\nOf course, the SCF is only one of the numerous available governmental data sets. And besides governmental data, there are also data sets from organizations like the World Bank, WHO, or Unicef. Some might be much easier to handle.\nAgain, a quick assessment.\nAdvantages:\n Commonly high data quality. Especially if the data has been used by other researchers or practitioners. Often data is well documented. Therefore, one can understand how the variables were created.  Disadvantages:\n It can take a lot of time to gather and transform features (think about the 45,000 lines of text). It can be hard to understand the data sets. Access is sometimes only given on a request basis.  Conclusion So what have we learned? Accessing data can be quite a hassle and takes time. Loading data and being ready to rumble? No, this is unrealistic.\nBut what collecting data the old way (not on Kaggle) ultimately does is that we must ask ourselves the right questions. We have to think before we have the data. Why? Because collecting data is a lot of effort. This means that we hopefully collect and use meaningful data for our analyses.\n Keep in mind that an analysis can only be as good as the quality of the data.\n If you have any questions or comments, feel free to reach me via the contact field or LinkedIn.\n","date":1591401600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1591401600,"objectID":"07062caeecce7938b1e0cb61ae79bfd3","permalink":"/post/how-to-collect-data-for-your-analysis/","publishdate":"2020-06-06T00:00:00Z","relpermalink":"/post/how-to-collect-data-for-your-analysis/","section":"post","summary":"Hint: Using a Kaggle data set might not be sufficient.","tags":["Data Science","Research"],"title":"How to Collect Data for Your Analysis","type":"post"},{"authors":null,"categories":null,"content":"This article was first published on LinkedIn.\nMoney is essential as a student, and time is limited, right?\nBut what if I told you that joining a student initiative has been one of the best decisions that I have made in the last year? Please hear me out. I know that your schedule is more than full. But maybe, only maybe, it‚Äôs a decision that is also right for you.\nWhat I want to share is my journey with a so-called student initiative. Not any student initiative. A student initiative driven by a specific purpose and great people.\nThe first time I heard about TechAcademy was through a friend. At that time, my life was hectic, so I asked some questions and probably quickly switched the topic. ‚ÄúGood for him that he has the time to start a new student initiative without getting paid for it‚Äù I thought to myself. But since I was too focused on my own life, I did not hear him out.\nSome months passed, my life changed dramatically, and of a sudden, I saw an announcement on LinkedIn that this initiative was finally ready to take off. This time, I was more than interested. Why? Because I recognized it had a clear purpose. And this purpose related to me.\nSo what does TechAcademy do? TechAcademy prepares students for the digital future. Each semester, they offer 70 students from all faculties the opportunity to acquire programming skills in the fields of Data Science and Web Development.\nI had learned some lines of code during my first semester, but I‚Äôve never had the opportunity to apply it. Without having any projects or people that were using this programming language, I quickly stopped. But remembering that I once wanted to improve my skills, I could directly see that they were offering a valuable program.\nSo what did I do? I quickly reached out to one of the founders and offered my help. Why? I don‚Äôt know. Maybe I wanted to meet some new people, and perhaps I wanted to be part of something bigger than me. As I said, I don‚Äôt know.\nFast forward one and a half years. What has happened in this time is fantastic. I got to meet so many inspiring people, learned a lot professionally, and we, as a team, have been able to have an impact on the lives of over 190 students.\nHere are the three things that I got from joining a student initiative. 1. Community When working at my student initiative, I got to meet a lot of people.\nFirst, there is my team with whom I organize everything. I won‚Äôt lie. Being in a student initiative involves a lot of work. But through that work and the shared purpose, I got close to my team. We see each other every week, which led to new friendships. And these people are cool, driven, and inspiring ‚Äî precisely the people I want to call friends.\nSecond, I met so many participants. Each and everyone is different. At TechAcademy, we are open to every faculty. Why? Because each and everyone lives in a digital world that is exponentially changing.\nLuckily, we do not have just business students that participate in our program. I mean, when and where do I meet mathematics, physics, teaching, or humanities students all in one place? I find it interesting to get to know different perspectives.\nI gratefully remember one Ph.D. student in archaeology that joined our program. She wanted to learn Python to write an algorithm to classify ancient vases. How crazy is that?!\n2. Professional experience I have learned so much in my ‚Äújob‚Äù at TechAcademy. It feels like working in a small startup. I need to fulfill specific responsibilities. If I don‚Äôt do it, I let my team down. There‚Äôs nobody else to blame. Therefore, I learned to deliver.\nAlso, I have the freedom to bring in my ideas. I can make a difference. We are 15 people right now. Therefore, my voice gets heard. And on my team, there are only three of us. We can do our job as we please. Nobody tells us how we should do things.\nWe can try out new methods to approach potential partners. We can try out new negotiation techniques. In the end, we can assess what works and use it from that point on. Compare that to work in a big corporation as an intern. There you don‚Äôt question the status quo. There you just do what your colleagues ask you to do.\nGenerally, I have learned much more at TechAcademy than in any internship. As I said, you have to make stuff happen. My responsibility is to create partnerships with companies. Through these partnerships, we can offer our participants valuable insights on how digitalization is lived in practice. Also, we can finance our initiative through these partnerships since our program is entirely free of charge for students.\nSo why have I learned much more than in any other internship? I approach companies, have initial phone calls and meetings with them. I negotiate contracts, and I have to maintain the relationship once the contract is signed. Most importantly, I need to make sure that we, as a student initiative, solve a problem for the company. If not, they won‚Äôt work with us.\nI compare this with two sales-related internships that I had. There I prepared (potential) customer information and conducted market research. If I got fortunate, I was allowed to join a call or a meeting as a ‚Äúfly on the wall‚Äù. I mean, I am thankful for the experience and that I was paid, but these are indeed two different worlds.\n3. Caring for others by giving back I don‚Äôt remember which book it was (probably one of Tony Robbin‚Äôs books), but there, the author made clear that happiness can be achieved through helping others. That there is nothing more fulfilling than going the extra mile for another human being. To step back from one‚Äôs ego and to care about someone who‚Äôs not yourself.\nI have found truth in these words. Of course, being in a student initiative has had a significant impact on my own life. But when you think about it, I am sacrificing my time so that students can participate in the best-possible program that we can create.\nThe students are enabled to learn a new programming language. For many of them, it‚Äôs the first time that they have used a program other than Word or Excel. They don‚Äôt know what machine learning or data science is. But when they participate in our program, a whole new world has opened to them.\nAlso, they meet so many people who become good friends. Not everyone is good at meeting new people, but we make sure that they have the opportunity to get out of their comfort zone. One friend told me last semester:\n \u0026ldquo;It is so hard for me to meet new people. Now, after having been here every other week, I have met people with whom I go to the canteen or have study groups.\u0026rdquo;\n I can assure you that this has had a significant impact on her life. I bet that many other stories are similar.\nMaybe someone finds a new passion. Another one creates a living out of that passion. Maybe one person can finally write his or her empirical bachelor or master thesis without getting nightmares. Another one finds friends for life.\nThis, by itself, is very motivating. I am part of something that creates value in the lives of many people. It‚Äôs not about me, it‚Äôs about them. And this is something that I love about my work at TechAcademy.\nIndeed, joining TechAcademy had a meaningful impact on my life. I am thankful for everyone that made this journey possible, for all the people that I have met, and all the memorable days that I have spent with this initiative.\nThe journey goes on, and maybe, just maybe, I have inspired you to look for opportunities at your university or your neighborhood. Of course, TechAcademy is not the only cool initiative there is.\nIf you have any questions or comments, feel free to reach me via the contact field or LinkedIn.\n","date":1590451200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1590451200,"objectID":"e7fc2daeb8628b946fa539529f680efe","permalink":"/post/working-for-free-improved-my-life-in-3-great-ways/","publishdate":"2020-05-26T00:00:00Z","relpermalink":"/post/working-for-free-improved-my-life-in-3-great-ways/","section":"post","summary":"Joining a student initiative proved to be the best decision that I made last year.","tags":["Personal Insights","Student Initiative"],"title":"Working for Free Improved My Life in 3 Great Ways","type":"post"},{"authors":null,"categories":null,"content":"This article was first published on Medium.\nLet\u0026rsquo;s face it. You\u0026rsquo;re an aspiring Data Scientist. You need to desperately solve a classification task either to impress your professor or to finally become one of these mysterious Kaggle winners. The problem is that you\u0026rsquo;ve got no time. The presentation or the Kaggle competition is due in 24 hours. Of course, you could stick to a simple logit regression and call it a day. \u0026ldquo;No, not this time!\u0026rdquo; you think, and you\u0026rsquo;re browsing the web to find THE SOLUTION. But it better be fast.\nOkay, this is a highly improbable event. However, I wanted to share an easy XGBoost implementation that proved to be lightning-fast (compared to other solutions) that can help you to achieve a more stable and accurate model.\nHyperparameter-tuning was the step in which I lost most of my \u0026ldquo;valuable\u0026rdquo; time. I had to choose between different options ranging from manually trying out different hyperparameter combinations to more advanced methods like grid or random search. I felt overwhelmed. Ultimately, it took quite a while to find a technique that fulfilled my needs. Therefore, the focus will be on this step of building an XGBoost model.\nWhy are we using XGBoost again? Because it rocks. It is an optimized distributed gradient boosting library that has been used with great success on many machine learning challenges. Among the advantages of XGBoost are that its models are easily scalable, tend to avoid overfitting, and can be used for a wide range of problems [1].\nOf course, only preparing the data and executing XGBoost would somehow work. But clearly, there is no free lunch in data science. This procedure would leave you with a model that is most likely overfitted and therefore performs badly on an out-of-sample data set. Meaning: you have created a useless model that you should not use with other data.\nLuckily, XGBoost offers several ways to make sure that the performance of the model is optimized. Hyperparameter-tuning is the last part of the model building and can increase your model\u0026rsquo;s performance. Tuning is a systematic and automated process of varying parameters to find the \u0026ldquo;best\u0026rdquo; model.\nIn my opinion, learning is best done through a reproducible example, so please feel free to copy the code and run it in RStudio (or any other IDE) if you are experiencing any troubles with my explanations. I assume a basic understanding of machine learning, so please use Google if anything is not clear.\nIn the first part, I build (together with you) a basic XGBoost model. Thus, we are loading, preparing, and transforming the data, and at the end of this section, we will have received our first results. In the second part, I discuss my experiences with several hyperparameter-tuning methods. For our final model, we will use a random search algorithm to increase the predictive accuracy of our binary classification task.\nLet\u0026rsquo;s get started.\nBuilding an XGBoost model with (mostly) default parameters I chose to use the famous Titanic data set from Kaggle. If you have a Kaggle account, you can get the data here. If you don\u0026rsquo;t have a Kaggle account, you can also check out my GitHub repository, where you\u0026rsquo;ll find the data sets and the scripts of this blog post. As an aspiring data scientist, you most likely have at least one of these accounts.\nOur task is to predict whether a passenger survived the tragic titanic incident or not‚Ää-‚Ääa typical binary classification task.\nFirst, we load the necessary packages and train.csv and test.csv. Then we can have a look at the structure of the data.\n\r'data.frame': 891 obs. of 12 variables:\r$ PassengerId: int 1 2 3 4 5 6 7 8 9 10 ...\r$ Survived : int 0 1 1 1 0 0 0 0 1 1 ...\r$ Pclass : int 3 1 3 1 3 3 1 3 3 2 ...\r$ Name : chr \u0026quot;Braund, Mr. Owen Harris\u0026quot; \u0026quot;Cumings, Mrs. John Bradley (Florence Briggs Thayer)\u0026quot; \u0026quot;Heikkinen, Miss. Laina\u0026quot; \u0026quot;Futrelle, Mrs. Jacques Heath (Lily May Peel)\u0026quot; ...\r$ Sex : chr \u0026quot;male\u0026quot; \u0026quot;female\u0026quot; \u0026quot;female\u0026quot; \u0026quot;female\u0026quot; ...\r$ Age : num 22 38 26 35 35 NA 54 2 27 14 ...\r$ SibSp : int 1 1 0 1 0 0 0 3 0 1 ...\r$ Parch : int 0 0 0 0 0 0 0 1 2 0 ...\r$ Ticket : chr \u0026quot;A/5 21171\u0026quot; \u0026quot;PC 17599\u0026quot; \u0026quot;STON/O2. 3101282\u0026quot; \u0026quot;113803\u0026quot; ...\r$ Fare : num 7.25 71.28 7.92 53.1 8.05 ...\r$ Cabin : chr \u0026quot;\u0026quot; \u0026quot;C85\u0026quot; \u0026quot;\u0026quot; \u0026quot;C123\u0026quot; ...\r$ Embarked : chr \u0026quot;S\u0026quot; \u0026quot;C\u0026quot; \u0026quot;S\u0026quot; \u0026quot;S\u0026quot; ...\r\u0026gt; str(test)\r'data.frame': 418 obs. of 11 variables:\r$ PassengerId: int 892 893 894 895 896 897 898 899 900 901 ...\r$ Pclass : int 3 3 2 3 3 3 3 2 3 3 ...\r$ Name : chr \u0026quot;Kelly, Mr. James\u0026quot; \u0026quot;Wilkes, Mrs. James (Ellen Needs)\u0026quot; \u0026quot;Myles, Mr. Thomas Francis\u0026quot; \u0026quot;Wirz, Mr. Albert\u0026quot; ...\r$ Sex : chr \u0026quot;male\u0026quot; \u0026quot;female\u0026quot; \u0026quot;male\u0026quot; \u0026quot;male\u0026quot; ...\r$ Age : num 34.5 47 62 27 22 14 30 26 18 21 ...\r$ SibSp : int 0 1 0 0 1 0 0 1 0 2 ...\r$ Parch : int 0 0 0 0 1 0 0 1 0 0 ...\r$ Ticket : chr \u0026quot;330911\u0026quot; \u0026quot;363272\u0026quot; \u0026quot;240276\u0026quot; \u0026quot;315154\u0026quot; ...\r$ Fare : num 7.83 7 9.69 8.66 12.29 ...\r$ Cabin : chr \u0026quot;\u0026quot; \u0026quot;\u0026quot; \u0026quot;\u0026quot; \u0026quot;\u0026quot; ...\r$ Embarked : chr \u0026quot;Q\u0026quot; \u0026quot;S\u0026quot; \u0026quot;Q\u0026quot; \u0026quot;S\u0026quot; ...\r After getting an overview, we need to perform data preparation and feature engineering tasks. I just used some common sense and looked for some examples on Kaggle. Feature engineering is not the focus of this post. Therefore, I stick to the basics. But be aware that feature engineering is generally a crucial step and can significantly enhance the performance of your model. If you are not writing a blog post about hyperparameter-tuning, then spend much more time on this step! If you need individual guidance with excellent explanations, have a look here.\nTo perform these tasks, we bind the rows of the train and test set into one data frame. XGBoost needs to be able to work with the data. Therefore, we need to one-hot encode the data sets into sparse matrices. This is done because XGBoost only works with numeric or integer variables. Since we have factor variables in our data sets, the following approach creates one column per feature level. So instead of one row for Pclass, we will obtain the rows Pclass1, Pclass2, and Pclass3. This is also the reason why I put train and test set into one data set. Previously, when performing these tasks independently, the structure of the data got messed up.\n\rAfter conducting the data preparation and the feature engineering tasks, we split train and test set again.\nAfter these steps, the structure of the train and test data set is the same. Only the Survived variable is not present in the test set.\n'data.frame': 891 obs. of 13 variables:\r$ Pclass1 : num 0 1 0 1 0 0 1 0 0 0 ...\r$ Pclass2 : num 0 0 0 0 0 0 0 0 0 1 ...\r$ Pclass3 : num 1 0 1 0 1 1 0 1 1 0 ...\r$ Sexmale : num 1 0 0 0 1 1 1 1 0 0 ...\r$ Age : num 22 38 26 35 35 NA 54 2 27 14 ...\r$ SibSp : num 1 1 0 1 0 0 0 3 0 1 ...\r$ Parch : num 0 0 0 0 0 0 0 1 2 0 ...\r$ Fare : num 7.25 71.28 7.92 53.1 8.05 ...\r$ EmbarkedC: num 0 1 0 0 0 0 0 0 0 1 ...\r$ EmbarkedQ: num 0 0 0 0 0 1 0 0 0 0 ...\r$ EmbarkedS: num 1 0 1 1 1 0 1 1 1 0 ...\r$ Family : num 1 1 0 1 0 0 0 4 2 1 ...\r$ Survived : int 0 1 1 1 0 0 0 0 1 1 ...\r\u0026gt; str(test)\r'data.frame': 418 obs. of 12 variables:\r$ Pclass1 : num 0 0 0 0 0 0 0 0 0 0 ...\r$ Pclass2 : num 0 0 1 0 0 0 0 1 0 0 ...\r$ Pclass3 : num 1 1 0 1 1 1 1 0 1 1 ...\r$ Sexmale : num 1 0 1 1 0 1 0 1 0 1 ...\r$ Age : num 34.5 47 62 27 22 14 30 26 18 21 ...\r$ SibSp : num 0 1 0 0 1 0 0 1 0 2 ...\r$ Parch : num 0 0 0 0 1 0 0 1 0 0 ...\r$ Fare : num 7.83 7 9.69 8.66 12.29 ...\r$ EmbarkedC: num 0 0 0 0 0 0 0 0 1 0 ...\r$ EmbarkedQ: num 1 0 1 0 0 0 1 0 0 0 ...\r$ EmbarkedS: num 0 1 0 1 1 1 0 1 0 1 ...\r$ Family : num 0 1 0 0 2 0 0 2 0 2 ...\r Remember the overfitting problem that I talked about? To obtain a better model, this problem needs to be prevented while ensuring that the model learns the generalized underlying relationships of the features with the target variable. So we use the train set and split it into a training and a validation set. We use 80% of the passengers for training the model and 20% for the validation of the model. Keep on waiting. I will explain what the validation set does in a few paragraphs.\n\rAfterward, we will create xgb.DMatrices. These are optimized matrices for XGBoost. The label attribute specifies the target variable. We cannot specify a label for the test set since we do not have any information about it.\n\rFinally, we can train our first XGBoost model. We use the xgboost.train() command, and the gradient booster tree and the objective of binary classification are specified. The maximum number of iterations is set to 1,000. After these settings, the boosting algorithm could be implemented without further specifications on the training set. When keeping the default values, the algorithm would create 1,000 iterations of the model.\nWhat is the problem here? Imagine you are setting the number of iterations to a high number. Then the model iterates forever, and in the end, you would get an accuracy of 100% on your training data. When testing this model on the test set, something strange would happen. ‚ÄúWhy do I have only [put any low number here]% accuracy?! I thought my model was perfect!‚Äù. True, perfect in the sense that it perfectly describes the underlying data of your training set. Generalizable on other data? No.\nTo prevent this behavior, we create the validation set that I have talked about. We use the training set to train a model. But instead of training it until eternity, we set the early_stopping_rounds parameter to 50. Setting this parameter is important because it will stop the training of the model when the accuracy of the validation set has not improved for the specified number of rounds. When this is the case, then the algorithm automatically chooses the iteration with the highest accuracy. This setting prevents our overfitting problem.\nIn the following figure, you can quickly see this behavior. I display accuracy (1-error) and AUC of an XGBoost model depending on the iteration for the training and the validation set. Following my explanations, the training set would quickly reach an accuracy of 96% after 60 rounds. Luckily, our algorithm detects that the validation accuracy is not improving after ten iterations and therefore stops the algorithm at this iteration. Interestingly, the AUC is improving until round 60, so when changing the evaluation metric, we would get another result. Therefore, a change in the evaluation metric can lead to different results. Since we want to optimize the accuracy of the model, we stick to accuracy as an evaluation metric.\n\rAccuracy and AUC by Iteration of a XGBoost Model. Plot created by author.\r\rOkay, now it‚Äôs time to reveal the default XGBoost algorithm. Specifying the watchlist is an important step here because it is the parameter that tells XGBoost to stop iterating when the validation accuracy (1-error) does not improve anymore.\n\rMultiple eval metrics are present. Will use val_error for early stopping.\rWill train until val_error hasn't improved in 50 rounds.\r[11] train-auc:0.959340 train-error:0.099719 val-auc:0.856732 val-error:0.139665 [21] train-auc:0.970900 train-error:0.085674 val-auc:0.858648 val-error:0.134078 [31] train-auc:0.981661 train-error:0.070225 val-auc:0.860495 val-error:0.145251 [41] train-auc:0.988527 train-error:0.051966 val-auc:0.865969 val-error:0.145251 [51] train-auc:0.992277 train-error:0.042135 val-auc:0.868979 val-error:0.156425 Stopping. Best iteration:\r[10] train-auc:0.959241 train-error:0.101124 val-auc:0.853243 val-error:0.128492\r We then predict Survival of the validation set with our model. We display our results with the confusionMatrix command. Ahhhh important information: this is only the result of our validation set! Right now, we want to get a feeling of how good our model COULD be. Please do not confuse it with the test that we will perform afterward with the test set.\n\rPrediction Not Survived Survived\rNot Survived 109 16\rSurvived 7 47\rAccuracy : 0.8715 95% CI : (0.8135, 0.9168)\rNo Information Rate : 0.648 P-Value [Acc \u0026gt; NIR] : 1.189e-11\rKappa : 0.7088\rMcnemar's Test P-Value : 0.09529 Sensitivity : 0.7460 Specificity : 0.9397 Pos Pred Value : 0.8704 Neg Pred Value : 0.8720 Prevalence : 0.3520 Detection Rate : 0.2626 Detection Prevalence : 0.3017 Balanced Accuracy : 0.8428 'Positive' Class : Survived\r What comes first to my mind is that our first model is especially great in predicting people that died (as seen by the specificity metric). To make a statement about how great our first attempt is, we must assess the balance (in our case imbalance) of the target variable. How many people have survived? How many people died? Quick calculations:\nP(prediction=0) = P(class=0) = 0.648\rP(prediction=1) = P(class=1) = 0.352\racc = P(class=0) * P(prediction=0) + P(class=1) * P(prediction=1)\r= (0.648 * 0.648) + (0.352 * 0.352)\r= 0.5438\r So what we can say is that our model performed 32.77 percentage points (0.8715‚Äì0.5438) better than a ‚Äúweighted guess‚Äù (thanks to this post for refreshing my knowledge). But as I said, this is only the result of the validation set which we used to optimize our model. So let‚Äôs test our default model by submitting it to Kaggle.\nAfter uploading the predictions of the test set to Kaggle, we get our first result. We reached an accuracy of 76.07%. So we lost around 11% compared to the validation accuracy‚Ä¶DAMN.\n \u0026ldquo;But maybe there is light at the end of the tunnel?! What if hyperparameter-tuning is the solution?!\u0026rdquo;\n Hyperparameter-Tuning of an XGBoost Model There are different approaches to select hyperparameters. Due to this high degree of choice, many users choose the values of hyperparameters based on reputation, intuitive appeal, or adhere to the default parameter values. This may result in a model whose potential has not been fully utilized [2]. Okay, what have we learned? A structured hyperparameter-tuning process can increase the potential of our model!\nWhat I certainly did was to look at these great posts to get a feeling of how I could do it: here and here.\nSome authors used an iterative manual search approach. They selected hyperparameter-values, ran the model, looked at the output, found some logic, and repeated the whole process. Some first optimized the learning rate, other the number of tree leaves, etc. For me, I could not find one clear routine and it is definitely a time-consuming process. Also, it does not seem to follow the definition of tuning since this process is not really systematic and automated.\nWay more advanced, I found hyperparameter-tuning procedures in the mlr and caret packages that make use of grid and random search procedures. But what are grid and random search? The grid method runs all possible combinations of predefined values for hyperparameters. The higher the number of discrete values for hyperparameters and the more hyperparameters, the more computationally expensive this method. To be specific, grid search creates an exponentially increasing number of models the more values and the more hyperparameters are used. Thus, it gets difficult to handle quite quickly. With random search, one only defines the search space and sets the number of models that have to be created.\nSo I had a look into the literature and found a great paper from Bergstra et al. (2012). Their research shows that the random search approach has a higher efficiency compared to grid trials and manual search when granting the same computation time. Also, contradictory to the manual search of hyperparameters, the results acquired through random search are reproducible [3]. NICE!\nActually, I build a random search algorithm with the mlr package, but I faced severe problems. Due to the complex structure of this algorithm, the process had long running times and I could only test around 150 models per hour. Definitely too slow for an algorithm that takes less around 1 second to execute. Sure, k-fold cross-validation sounds really cool and provides more reliable results, but I just was not patient enough. Luckily, we have out-of-bag observations as provided in the test set through which we can evaluate the model performance of the trained algorithm. Also, testing my results became an issue. I could just not say with certainty that I created the algorithm with the right settings. This was due to the fact that I could not compare the results of the algorithm and a simple XGBoost model with the same hyperparameter-values because of k-fold cross-Validation.\nSo what did I do? In the end, I just created 10,000 random hyperparameter-value sets within a given search space with a for-loop. Then I executed the XGBoost algorithm 10,000 times with the predefined hyperparameter value sets. Of course, I saved the hyperparameter values and the corresponding validation accuracy as a csv-file. Specifying a seed before the random creation of the hyperparameters and the search algorithm ensured that the results are reproducible.\n\rTo evaluate my approach, I asked myself the following questions:\n  Do I understand my procedure? Yes.\n  Can I explain this approach to my professor or my fellow Kaggle competitors? Yes.\n  How long did it take to compare 10,000 XGBoost models with differing hyperparameter values? 29.7 minutes with a standard consumer laptop. Is this fast? Yes. Could I increase the speed with parallel computing if I wanted? Yes.\n  Can I state that I have found the best model? No. Although a high number of models are created, it is clear that the described approach has not found the very best model performance. To obtain better results, one could alter the search space or even include more hyperparameters. Still, finding the perfect model could not be stated after additional random searches with changed settings.\n  Does that procedure lead to the most stable model there is? Most likely not. As I said, using other methods (k-fold cross-validation, stratification, etc.) could lead to a more stable model.\n  So in the end, I decided on a procedure that is both fast and easy.\nLet‚Äôs have a look at the random search table.\n\r\rValues are rounded. If only copied like this into the model, you will obtain different results.\nIndeed, hyperparameter tuning has a strong effect on the performance of the model. The validation accuracy ranges between 80.4 percent and 89.4 percent, with a median of 86.6 percent and a mean of 86.7 percent. Remember, the validation accuracy that we got from an XGBoost model with default values was 87.2 percent‚Ä¶\nTo see more details, we plug the hyperparameters of the best hyperparameter value set into our XGBoost algorithm and again have a look at the model‚Äôs statistics and the confusion matrix.\n\rPrediction Not Survived Survived\rNot Survived 110 13\rSurvived 6 50\rAccuracy : 0.8939 95% CI : (0.8392, 0.9349)\rNo Information Rate : 0.648 P-Value [Acc \u0026gt; NIR] : 4.303e-14\rKappa : 0.7612\rMcnemar's Test P-Value : 0.1687 Sensitivity : 0.7937 Specificity : 0.9483 Pos Pred Value : 0.8929 Neg Pred Value : 0.8943 Prevalence : 0.3520 Detection Rate : 0.2793 Detection Prevalence : 0.3128 Balanced Accuracy : 0.8710 'Positive' Class : Survived\r Most significant improvement: sensitivity has increased by 4.77 percentage points. Having better validation results is great, but in the end, only our test accuracy score is essential. Now, the final moment has come‚Ä¶\n\rWe get an accuracy of 77.99%. This is better than the accuracy of our base model. But how have we done against other Kaggler‚Äôs? Hmmm‚Ä¶it doesn‚Äôt look too good for our model, as we can see in the below plot. We are only under the top 37%. That‚Äôs why we come to the final learning of my blog post. Hyperparameter-tuning is the last part of the model creation process. Since I got too excited about my search algorithm, I did not put enough time in feature engineering my model. Without useful features that the model can learn from, I can do all the hyperparameter-tuning I want. It will still be just mediocre. Therefore, when time is limited, one should focus on feature engineering and not on hyperparameter-tuning.\n\rHistogram of the public leaderboard scores. The models that have had better accuracy than our model are displayed in black, the others are displayed in grey. We would have gotten an accuracy of 0.627 if we had predicted that everyone died. An accuracy of 0.766 would have been achieved if we had predicted that all males over the age of three died and that the women and children survived. Plot created by author.\r\rWere all the efforts for nothing? Of course not. If you have enough time, you can create great features and use a hyperparameter-tuning process (in the end!) that helps you to extract the very last accuracy percentage points out of your model.\nConclusion What I have introduced here is a procedure that I used to tune the model that I created for a seminar paper. Although plenty of information can be found via Google, I struggled to find a suitable solution for my needs. What I wanted, in the end, was something that I could correctly understand and which did not require endless hours to compute. Also, I wanted an automated approach whose results I could replicate at any point in time. These criteria are definitely fulfilled by my simple random search algorithm.\nI don‚Äôt claim that this procedure leads to the best or the most stable model. If you had the pleasure to stumble upon a better solution, have any questions or comments, feel free to reach me via the contact field or LinkedIn.\nReferences [1] Chen, T. \u0026amp; Guestrin, C. (2016), Xgboost: A scalable tree boosting system, in B. Krishnapuram \u0026amp; M. Shah, eds, ‚ÄòKDD ‚Äô16: Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining‚Äô, pp. 785‚Äì794.\n[2] Thornton, C., Hutter, F., Hoos, H. H. \u0026amp; Leyton-Brown, K. (2013), Autoweka: Combined selection and hyperparameter optimization of classification algorithms, in R. Ghani, T. E. Senator, P. Bradley, R. Parekh \u0026amp; J. He, eds, ‚ÄòKDD ‚Äô13: Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining‚Äô, pp. 847‚Äì855.\n[3] Bergstra, J. \u0026amp; Bengio, Y. (2012), ‚ÄòRandom search for hyper-parameter optimization‚Äô, Journal of Machine Learning Research 12, 281‚Äì305.\n","date":1590105600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1590105600,"objectID":"e5972d97568ada12e6c8494737a15276","permalink":"/post/getting-to-a-hyperparameter-tuned-xgboost-model-in-no-time/","publishdate":"2020-05-22T00:00:00Z","relpermalink":"/post/getting-to-a-hyperparameter-tuned-xgboost-model-in-no-time/","section":"post","summary":"How to fine-tune your first XGBoost model in R with random search.","tags":["Data Science"],"title":"Getting to a Hyperparameter-Tuned XGBoost Model in No Time","type":"post"},{"authors":null,"categories":null,"content":"Wir informieren Sie nachfolgend gem√§√ü den gesetzlichen Vorgaben des Datenschutzrechts (insb. gem√§√ü BDSG n.F. und der europ√§ischen Datenschutz-Grundverordnung ‚ÄöDS-GVO‚Äò) √ºber die Art, den Umfang und Zweck der Verarbeitung personenbezogener Daten durch unser Unternehmen.¬†Diese Datenschutzerkl√§rung gilt auch f√ºr unsere Websites und Sozial-Media-Profile. Bez√ºglich der Definition von Begriffen wie etwa ‚Äûpersonenbezogene Daten‚Äú oder ‚ÄûVerarbeitung‚Äú verweisen wir auf Art. 4 DS-GVO.\nName und Kontaktdaten des Verantwortlichen\nUnser Verantwortlicher (nachfolgend ‚ÄûVerantwortlicher‚Äú) i.S.d. Art. 4 Zif. 7 DS-GVO ist:\nJonathan Ratschat\nKronberger Stra√üe 43\n60323, Frankfurt am Main, Deutschland\nE-Mail-Adresse: j.ratschat@protonmail.com\nDatenarten, Zwecke der Verarbeitung und Kategorien betroffener Personen\nNachfolgend informieren wir Sie √ºber Art, Umfang und Zweck der Erhebung, Verarbeitung und Nutzung personenbezogener Daten.\n1. Arten der Daten, die wir verarbeiten\nNutzungsdaten (Zugriffszeiten, besuchte Websites etc.), Kontaktdaten (Telefonnummer, E-Mail, Fax etc.) 2. Zwecke der Verarbeitung nach Art. 13 Abs. 1 c) DS-GVO\nWebsite technisch und wirtschaftlich optimieren, Leichten Zugang zur Website erm√∂glichen, Optimierung und statistische Auswertung unserer Dienste, Nutzererfahrung verbessern, Website benutzerfreundlich gestalten, Kontaktanfragen abwickeln. 3. Kategorien der betroffenen Personen nach Art. 13 Abs. 1 e) DS-GVO\nBesucher/Nutzer der Website. Die betroffenen Personen werden zusammenfassend als ‚ÄûNutzer‚Äú bezeichnet.\nRechtsgrundlagen der Verarbeitung personenbezogener Daten\nNachfolgend Informieren wir Sie √ºber die Rechtsgrundlagen der Verarbeitung personenbezogener Daten:\nWenn wir Ihre Einwilligung f√ºr die Verarbeitung personenbezogenen Daten eingeholt haben, ist Art. 6 Abs. 1 S. 1 lit. a) DS-GVO Rechtsgrundlage.\rIst die Verarbeitung zur Erf√ºllung eines Vertrags oder zur Durchf√ºhrung vorvertraglicher Ma√ünahmen erforderlich, die auf Ihre Anfrage hin erfolgen, so ist Art. 6 Abs. 1 S. 1 lit. b) DS-GVO Rechtsgrundlage.\rIst die Verarbeitung zur Erf√ºllung einer rechtlichen Verpflichtung erforderlich, der wir unterliegen (z.B. gesetzliche Aufbewahrungspflichten), so ist Art. 6 Abs. 1 S. 1 lit. c) DS-GVO Rechtsgrundlage.\rIst die Verarbeitung erforderlich, um lebenswichtige Interessen der betroffenen Person oder einer anderen nat√ºrlichen Person zu sch√ºtzen, so ist Art. 6 Abs. 1 S. 1 lit. d) DS-GVO Rechtsgrundlage.\rIst die Verarbeitung zur Wahrung unserer oder der berechtigten Interessen eines Dritten erforderlich und √ºberwiegen diesbez√ºglich Ihre Interessen oder Grundrechte und Grundfreiheiten nicht, so ist Art. 6 Abs. 1 S. 1 lit. f) DS-GVO Rechtsgrundlage.\r\rWeitergabe personenbezogener Daten an Dritte und Auftragsverarbeiter\rOhne Ihre Einwilligung geben wir grunds√§tzlich keine Daten an Dritte weiter. Sollte dies doch der Fall sein, dann erfolgt die Weitergabe auf der Grundlage der zuvor genannten Rechtsgrundlagen z.B. bei der Weitergabe von Daten an Online-Paymentanbieter zur Vertragserf√ºllung oder aufgrund gerichtlicher Anordnung oder wegen einer gesetzlichen Verpflichtung zur Herausgabe der Daten zum Zwecke der Strafverfolgung, zur Gefahrenabwehr oder zur Durchsetzung der Rechte am geistigen Eigentum.\nWir setzen zudem Auftragsverarbeiter (externe Dienstleister z.B. zum Webhosting unserer Websites und Datenbanken) zur Verarbeitung Ihrer Daten ein. Wenn im Rahmen einer Vereinbarung zur Auftragsverarbeitung an die Auftragsverarbeiter Daten weitergegeben werden, erfolgt dies immer nach Art. 28 DS-GVO. Wir w√§hlen dabei unsere Auftragsverarbeiter sorgf√§ltig aus, kontrollieren diese regelm√§√üig und haben uns ein Weisungsrecht hinsichtlich der Daten einr√§umen lassen. Zudem m√ºssen die Auftragsverarbeiter geeignete technische und organisatorische Ma√ünahmen getroffen haben und die Datenschutzvorschriften gem. BDSG n.F. und DS-GVO einhalten\n\nDaten√ºbermittlung in Drittstaaten\nDurch die Verabschiedung der europ√§ischen Datenschutz-Grundverordnung (DS-GVO) wurde eine einheitliche Grundlage f√ºr den Datenschutz in Europa geschaffen. Ihre Daten werden daher vorwiegend durch Unternehmen verarbeitet, f√ºr die DS-GVO Anwendung findet. Sollte doch die Verarbeitung durch Dienste Dritter au√üerhalb der Europ√§ischen Union oder des Europ√§ischen Wirtschaftsraums stattfinden, so m√ºssen diese die besonderen Voraussetzungen der Art. 44 ff. DS-GVO erf√ºllen. Das bedeutet, die Verarbeitung erfolgt aufgrund besonderer Garantien, wie etwa die von der EU-Kommission offiziell anerkannte Feststellung eines der EU entsprechenden Datenschutzniveaus oder der Beachtung offiziell anerkannter spezieller vertraglicher Verpflichtungen, der so genannten ‚ÄûStandardvertragsklauseln‚Äú. Bei US-Unternehmen erf√ºllt die Unterwerfung unter das sog. ‚ÄûPrivacy-Shield‚Äú, dem Datenschutzabkommen zwischen der EU und den USA, diese Voraussetzungen.\n\nL√∂schung von Daten und Speicherdauer\nSofern nicht in dieser Datenschutzerkl√§rung ausdr√ºcklich angegeben, werden Ihre personenbezogen Daten gel√∂scht oder gesperrt, sobald die zur Verarbeitung erteilte Einwilligung von Ihnen widerrufen wird oder der Zweck f√ºr die Speicherung entf√§llt bzw. die Daten f√ºr den Zweck nicht mehr erforderlich sind, es sei denn deren weitere Aufbewahrung ist zu Beweiszwecken erforderlich oder dem stehen gesetzliche Aufbewahrungspflichten entgegenstehen. Darunter fallen etwa handelsrechtliche Aufbewahrungspflichten von Gesch√§ftsbriefen nach ¬ß 257 Abs. 1 HGB (6 Jahre) sowie steuerrechtliche Aufbewahrungspflichten nach ¬ß 147 Abs. 1 AO von Belegen (10 Jahre). Wenn die vorgeschriebene Aufbewahrungsfrist abl√§uft, erfolgt eine Sperrung oder L√∂schung Ihrer Daten, es sei denn die Speicherung ist weiterhin f√ºr einen Vertragsabschluss oder zur Vertragserf√ºllung erforderlich.\n\rBestehen einer automatisierten Entscheidungsfindung\rWir setzen keine automatische Entscheidungsfindung oder ein Profiling ein.\n\rBereitstellung unserer Website und Erstellung von Logfiles\rWenn Sie unsere Webseite lediglich informatorisch nutzen (also keine Registrierung und auch keine anderweitige √úbermittlung von Informationen), erheben wir nur die personenbezogenen Daten, die Ihr Browser an unseren Server √ºbermittelt. Wenn Sie unsere Website betrachten m√∂chten, erheben wir die folgenden Daten:\n‚Ä¢ IP-Adresse;\n‚Ä¢ Internet-Service-Provider des Nutzers;\n‚Ä¢ Datum und Uhrzeit des Abrufs;\n‚Ä¢ Browsertyp;\n‚Ä¢ Sprache und Browser-Version;\n‚Ä¢ Inhalt des Abrufs;\n‚Ä¢ Zeitzone;\n‚Ä¢ Zugriffsstatus/HTTP-Statuscode;\n‚Ä¢ Datenmenge;\n‚Ä¢ Websites, von denen die Anforderung kommt;\n‚Ä¢ Betriebssystem.\nEine Speicherung dieser Daten zusammen mit anderen personenbezogenen Daten von Ihnen findet nicht statt.\n\rDiese Daten dienen dem Zweck der nutzerfreundlichen, funktionsf√§higen und sicheren Auslieferung unserer Website an Sie mit Funktionen und Inhalten sowie deren Optimierung und statistischen Auswertung.\n\rRechtsgrundlage hierf√ºr ist unser in den obigen Zwecken auch liegendes berechtigtes Interesse an der Datenverarbeitung nach Art. 6 Abs. 1 S.1 lit. f) DS-GVO.\n\rWir speichern aus Sicherheitsgr√ºnden diese Daten in Server-Logfiles f√ºr die Speicherdauer von Tagen. Nach Ablauf dieser Frist werden diese automatisch gel√∂scht, es sei denn wir ben√∂tigen deren Aufbewahrung zu Beweiszwecken bei Angriffen auf die Serverinfrastruktur oder anderen Rechtsverletzungen.\n\r\r\rCookies\rWir verwenden sog. Cookies bei Ihrem Besuch unserer Website. Cookies sind kleine Textdateien, die Ihr Internet-Browser auf Ihrem Rechner ablegt und speichert. Wenn Sie unsere Website erneut aufrufen, geben diese Cookies Informationen ab, um Sie automatisch wiederzuerkennen. Zu den Cookies z√§hlen auch die sog. ‚ÄûNutzer-IDs‚Äú, wo Angaben der Nutzer mittels pseudonymisierter Profile gespeichert werden. Wir informieren Sie dazu beim Aufruf unserer Website mittels eines Hinweises auf unsere Datenschutzerkl√§rung √ºber die Verwendung von Cookies zu den zuvor genannten Zwecken und wie Sie dieser widersprechen bzw. deren Speicherung verhindern k√∂nnen (‚ÄûOpt-out‚Äú).\nEs werden folgende Cookie-Arten unterschieden:\n‚Ä¢ Notwendige, essentielle Cookies: Essentielle Cookies sind Cookies, die zum Betrieb der Webseite unbedingt erforderlich sind, um bestimmte Funktionen der Webseite wie Logins, Warenkorb oder Nutzereingaben z.B. bzgl. Sprache der Webseite zu speichern.\n‚Ä¢ Session-Cookies: Session-Cookies werden zum Wiedererkennen mehrfacher Nutzung eines Angebots durch denselben Nutzer (z.B. wenn Sie sich eingeloggt haben zur Feststellung Ihres Login-Status) ben√∂tigt. Wenn Sie unsere Seite erneut aufrufen, geben diese Cookies Informationen ab, um Sie automatisch wiederzuerkennen. Die so erlangten Informationen dienen dazu, unsere Angebote zu optimieren und Ihnen einen leichteren Zugang auf unsere Seite zu erm√∂glichen. Wenn Sie den Browser schlie√üen oder Sie sich ausloggen, werden die Session-Cookies gel√∂scht.\n‚Ä¢ Persistente Cookies: Diese Cookies bleiben auch nach dem Schlie√üen des Browsers gespeichert. Sie dienen zur Speicherung des Logins, der Reichweitenmessung und zu Marketingzwecken. Diese werden automatisiert nach einer vorgegebenen Dauer gel√∂scht, die sich je nach Cookie unterscheiden kann. In den Sicherheitseinstellungen Ihres Browsers k√∂nnen Sie die Cookies jederzeit l√∂schen.\n‚Ä¢ Cookies von Drittanbietern (Third-Party-Cookies insb. von Werbetreibenden): Entsprechend Ihren W√ºnschen k√∂nnen Sie Ihre Browser-Einstellung konfigurieren und z. B. Die Annahme von Third-Party-Cookies oder allen Cookies ablehnen. Wir weisen Sie jedoch an dieser Stelle darauf hin, dass Sie dann eventuell nicht alle Funktionen dieser Website nutzen k√∂nnen. Lesen Sie N√§heres zu diesen Cookies bei den jeweiligen Datenschutzerkl√§rungen zu den Drittanbietern.\n\rDatenkategorien: Nutzerdaten, Cookie, Nutzer-ID (inb. die besuchten Seiten, Ger√§teinformationen, Zugriffszeiten und IP-Adressen).\n\rZwecke der Verarbeitung: Die so erlangten Informationen dienen dem Zweck, unsere Webangebote technisch und wirtschaftlich zu optimieren und Ihnen einen leichteren und sicheren Zugang auf unsere Website zu erm√∂glichen.\n\rRechtsgrundlagen: Wenn wir Ihre personenbezogenen Daten mit Hilfe von Cookies aufgrund Ihrer Einwilligung verarbeiten (‚ÄûOpt-in‚Äú), dann ist Art. 6 Abs. 1 S. 1 lit. a) DSGVO die Rechtsgrundlage. Ansonsten haben wir ein berechtigtes Interesse an der effektiven Funktionalit√§t, Verbesserung und wirtschaftlichen Betrieb der Website, so dass in dem Falle Art. 6 Abs. 1 S. 1 lit. f) DS-GVO Rechtsgrundlage ist. Rechtsgrundlage ist zudem Art. 6 Abs. 1 S. 1 lit. b) DS-GVO, wenn die Cookies zur Vertragsanbahnung z.B. bei Bestellungen gesetzt werden.\n\rSpeicherdauer/ L√∂schung: Die Daten werden gel√∂scht, sobald sie f√ºr die Erreichung des Zweckes ihrer Erhebung nicht mehr erforderlich sind. Im Falle der Erfassung der Daten zur Bereitstellung der Website ist dies der Fall, wenn die jeweilige Session beendet ist.\nCookies werden ansonsten auf Ihrem Computer gespeichert und von diesem an unsere Seite √ºbermittelt. Daher haben Sie als Nutzer auch die volle Kontrolle √ºber die Verwendung von Cookies. Durch eine √Ñnderung der Einstellungen in Ihrem Internetbrowser k√∂nnen Sie die √úbertragung von Cookies deaktivieren oder einschr√§nken. Bereits gespeicherte Cookies k√∂nnen jederzeit gel√∂scht werden. Dies kann auch automatisiert erfolgen. Werden Cookies f√ºr unsere Website deaktiviert, k√∂nnen m√∂glicherweise nicht mehr alle Funktionen der Website vollumf√§nglich genutzt werden.\nHier finden Sie Informationen zur L\u0026ouml;schung von Cookies nach Browsern:\nChrome: https://support.google.com/chrome/answer/95647\nSafari: https://support.apple.com/de-at/guide/safari/sfri11471/mac\nFirefox: https://support.mozilla.org/de/kb/cookies-und-website-daten-in-firefox-loschen\nInternet Explorer: https://support.microsoft.com/de-at/help/17442/windows-internet-explorer-delete-manage-cookies\nMicrosoft Edge: https://support.microsoft.com/de-at/help/4027947/windows-delete-cookies\n\tWiderspruch und ‚ÄûOpt-Out‚Äú: Das Speichern von Cookies auf Ihrer Festplatte k√∂nnen Sie unabh√§ngig von einer Einwilligung oder gesetzlichen Erlaubnis allgemein verhindern, indem Sie in Ihren Browser-Einstellungen ‚Äûkeine Cookies akzeptieren‚Äú w√§hlen. Dies kann aber eine Funktionseinschr√§nkung unserer Angebote zur Folge haben. Sie k√∂nnen dem Einsatz von Cookies von Drittanbietern zu Werbezwecken √ºber ein sog. ‚ÄûOpt-out‚Äú √ºber diese amerikanische Website¬†(https://optout.aboutads.info)¬†oder diese europ√§ische Website¬†(http://www.youronlinechoices.com/de/praferenzmanagement/) widersprechen.\n\r\r\rNutzung der Blog-Funktionen / Kommentare\rSie k√∂nnen in unserem Blog, der Beitr√§ge zu Themen unserer Website enth√§lt, √∂ffentliche Kommentare abgeben. Sie k√∂nnen ein Pseudonym statt eines Klarnamens verwenden. Ihr Beitrag wird dann unter dem Pseudonym ver√∂ffentlicht. Die Angabe der E-Mail-Adresse ist Pflicht, alle sonstigen Informationen sind freiwillig.\n\rWir speichern bei Ihrer Einstellung eines Kommentars Ihre IP-Adresse mit Datum und Uhrzeit, welche wir nach Tagen l√∂schen. Die Speicherung dient dem berechtigten Interesse der Verteidigung gegen die Inanspruchnahme Dritter bei der Ver√∂ffentlichung rechtswidriger oder unwahrer Inhalte durch Sie. Ihre E-Mail-Adresse speichern wir zum Zweck der Kontaktaufnahme falls Dritte Ihre Kommentare juristisch beanstanden sollten.\n\rRechtsgrundlagen sind Art. 6 Abs. 1 S. 1 lit. b) und f) DS-GVO.\n\rVor der Ver√∂ffentlichung pr√ºfen wir Ihre Kommentare nicht. Im Falle von Beanstandungen Dritter behalten wir uns ein L√∂schrecht hinsichtlich Ihrer Kommentare vor. Wir geben die Daten nicht an Dritte weiter, es sei denn es ist notwendig zur Verfolgung unserer Anspr√ºche oder es besteht eine gesetzliche Verpflichtung (Art. 6 Abs. 1 S. 1. lit. c) DS-GVO).\n\rDie Daten werden gel√∂scht, sobald sie f√ºr die Erreichung des Zweckes ihrer Erhebung bzw. die Durchf√ºhrung des Vertrages nicht mehr erforderlich sind, weil der Vertrag beendet wurde.\n\r\r\rKontaktaufnahme per Kontaktformular / E-Mail / Fax / Post\rBei der Kontaktaufnahme mit uns per Kontaktformular, Fax, Post oder E-Mail werden Ihre Angaben zum Zwecke der Abwicklung der Kontaktanfrage verarbeitet.\n\rRechtsgrundlage f√ºr die Verarbeitung der Daten ist bei Vorliegen einer Einwilligung von Ihnen Art.¬†6 Abs.¬†1 S. 1 lit.¬†a) DS-GVO. Rechtsgrundlage f√ºr die Verarbeitung der Daten, die im Zuge einer Kontaktanfrage oder E-Mail, eines Briefes oder Faxes √ºbermittelt werden, ist Art.¬†6 Abs.¬†1 S. 1 lit.¬†f) DS-GVO. Der Verantwortliche hat ein berechtigtes Interesse an der Verarbeitung und Speicherung der Daten, um Anfragen der Nutzer beantworten zu k√∂nnen, zur Beweissicherung aus Haftungsgr√ºnden und um ggf. seiner gesetzlichen Aufbewahrungspflichten bei Gesch√§ftsbriefen nachkommen zu k√∂nnen. Zielt der Kontakt auf den Abschluss eines Vertrages ab, so ist zus√§tzliche Rechtsgrundlage f√ºr die Verarbeitung Art.¬†6 Abs.¬†1 S. 1 lit.¬†b) DS-GVO.\n\rWir k√∂nnen Ihre Angaben und Kontaktanfrage in unserem Customer-Relationship-Management System (\"CRM System\") oder einem vergleichbaren System speichern.\n\rDie Daten werden gel√∂scht, sobald sie f√ºr die Erreichung des Zweckes ihrer Erhebung nicht mehr erforderlich sind. F√ºr die personenbezogenen Daten aus der Eingabemaske des Kontaktformulars und diejenigen, die per E-Mail √ºbersandt wurden, ist dies dann der Fall, wenn die jeweilige Konversation mit Ihnen beendet ist. Beendet ist die Konversation dann, wenn sich aus den Umst√§nden entnehmen l√§sst, dass der betroffene Sachverhalt abschlie√üend gekl√§rt ist. Anfragen von Nutzern, die √ºber einen Account bzw. Vertrag mit uns verf√ºgen, speichern wir bis zum Ablauf von zwei Jahren nach Vertragsbeendigung. Im Fall von gesetzlichen Archivierungspflichten erfolgt die L√∂schung nach deren Ablauf: Ende handelsrechtlicher (6 Jahre) und steuerrechtlicher (10 Jahre) Aufbewahrungspflicht.\n\rSie haben jederzeit die M√∂glichkeit, die Einwilligung nach Art. 6 Abs. 1 S. 1 lit. a) DS-GVO zur Verarbeitung der personenbezogenen Daten zu widerrufen. Nehmen Sie per E-Mail Kontakt mit uns auf, so k√∂nnen Sie der Speicherung der personenbezogenen Daten jederzeit widersprechen.\n\r\r\rGoogle Analytics\rWir haben das Webseitenanalyse-Tool ‚ÄûGoogle Analytics‚Äú (Dienstanbieter: Google Ireland Limited, Registernr.: 368047, Gordon House, Barrow Street, Dublin 4, Irland) auf unserer Website integriert.\n\rDatenkategorien und Beschreibung der Datenverarbeitung: User-ID, IP-Adresse (anonymisiert). Beim Besuch unserer Website setzt Google einen Cookie auf Ihren Computer, um die Benutzung unserer Website durch Sie analysieren zu k\u0026ouml;nnen. Wir haben die IP-Anonymisierung \u0026bdquo;anonymizeIP\u0026ldquo; aktiviert, wodurch die IP-Adressen nur gek\u0026uuml;rzt weiterverarbeitet werden. Auf dieser Webseite wird Ihre IP-Adresse von Google daher innerhalb von Mitgliedstaaten der Europ\u0026auml;ischen Union oder in anderen Vertragsstaaten des Abkommens \u0026uuml;ber den Europ\u0026auml;ischen Wirtschaftsraum zuvor gek\u0026uuml;rzt. Nur in Ausnahmef\u0026auml;llen wird die volle IP-Adresse an einen Server von Google in den USA \u0026uuml;bertragen und dort gek\u0026uuml;rzt. Im Auftrag des Betreibers dieser Webseite wird Google diese Informationen benutzen, um Ihre Nutzung der Webseite auszuwerten, um Reports \u0026uuml;ber die Webseitenaktivit\u0026auml;ten zusammenzustellen und um weitere, mit der Websitenutzung und der Internetnutzung verbundene, Dienstleistungen gegen\u0026uuml;ber dem Verantwortlichen zu erbringen. Wir haben dar\u0026uuml;ber hinaus die ger\u0026auml;te\u0026uuml;bergreifende Analyse von Website-Besuchern aktiviert, die \u0026uuml;ber eine sog. User-ID durchgef\u0026uuml;hrt wird. Die im Rahmen von Google Analytics von Ihrem Browser \u0026uuml;bermittelte IP-Adresse wird nicht mit anderen Daten von Google zusammengef\u0026uuml;hrt. Weitere Informationen zu Datennutzung bei Google Analytics finden Sie hier:\u0026nbsp;https://www.google.com/analytics/terms/de.html\u0026nbsp;(Nutzungsbedingungen von Analytics),\u0026nbsp;https://support.google.com/analytics/answer/6004245?hl=de\u0026nbsp;(Hinweise zum Datenschutz bei Analytics) und Googles Datenschutzerkl\u0026auml;rung\u0026nbsp;https://policies.google.com/privacy.\n\rZweck der Verarbeitung: Die Nutzung von Google Analytics dient dem Zweck der Analyse, Optimierung und Verbesserung unserer Website.\n\rRechtsgrundlagen: Haben Sie f√ºr Verarbeitung Ihrer personenbezogenen Daten mittels ‚ÄûGoogle Analytics‚Äú vom Drittanbieter Ihre Einwilligung erteilt (‚ÄûOpt-in‚Äú), dann ist Art. 6 Abs. 1 S. 1 lit. a) DS-GVO die Rechtsgrundlage. Rechtsgrundlage ist zudem unser in den obigen Zwecken liegendes berechtigtes Interesse (der Analyse, Optimierung und Verbesserung unserer Website) an der Datenverarbeitung nach Art. 6 Abs. 1 S.1 lit. f) DS-GVO. Bei Services, die im Zusammenhang mit einem Vertrag erbracht werden, erfolgt das Tracking und die Analyse des Nutzerhaltens nach Art. 6 Abs. 1 S. 1 lit. b) DS-GVO, um mit den dadurch gewonnen Informationen, optimierte Services zur Erf√ºllung des Vertragszwecks anbieten zu k√∂nnen.\n\rSpeicherdauer: Die von uns gesendeten und mit Cookies, Nutzerkennungen (z. B. User-ID) oder Werbe-IDs verkn√ºpften Daten werden nach Monaten automatisch gel√∂scht. Die L√∂schung von Daten, deren Aufbewahrungsdauer erreicht ist, erfolgt automatisch einmal im Monat.\n\rDaten√ºbermittlung/Empf√§ngerkategorie: Google, Irland und USA. Die gewonnenen Daten werden in die USA \u0026uuml;bertragen und dort gespeichert. Falls personenbezogen Daten in die USA \u0026uuml;bertragen werden sollten, bietet die Zertifizierung Googles gem\u0026auml;\u0026szlig; Privacy-Shield-Abkommen (https://www.privacyshield.gov/EU-US-Framework) die Garantie daf\u0026uuml;r, dass das europ\u0026auml;ische Datenschutzrecht eingehalten wird. Wir haben zudem mit Google eine Vereinbarung zur Auftragsverarbeitung nach Art. 28 DS-GVO geschlossen.\n\rWiderspruchs- und Beseitigungsm√∂glichkeiten (‚ÄûOpt-Out‚Äú):\n\u0026bull;Das Speichern von Cookies auf Ihrer Festplatte k\u0026ouml;nnen Sie allgemein verhindern, indem Sie in Ihren Browser-Einstellungen \u0026bdquo;keine Cookies akzeptieren\u0026ldquo; w\u0026auml;hlen. Dies kann aber eine Funktionseinschr\u0026auml;nkung unserer Angebote zur Folge haben. Sie k\u0026ouml;nnen dar\u0026uuml;ber hinaus die Erfassung der, durch das Cookie erzeugten und auf Ihre Nutzung der Website bezogenen, Daten an Google sowie die Verarbeitung dieser Daten durch Google verhindern, indem sie das unter dem folgenden Link verf\u0026uuml;gbare Browser-Plugin herunterladen und installieren:\u0026nbsp;http://tools.google.com/dlpage/gaoptout?hl=de\n\u0026bull;Als Alternative zum obigen Browser-Plugin k√∂nnen Sie die Erfassung durch Google Analytics unterbinden, indem Sie¬†[__hier bitte__den Analytics Opt-Out Link Ihrer Webseite einf√ºgen]¬†klicken. Durch den Klick wird ein ‚ÄûOpt-out‚Äú-Cookie gesetzt, das die Erfassung Ihrer Daten beim Besuch dieser Webseite zuk√ºnftig verhindert. Dieses Cookie gilt nur f√ºr unsere Webseite und Ihren aktuellen Browser und hat nur solange Bestand bis Sie Ihre Cookies l√∂schen. In dem Falle m√ºssten Sie das Cookie erneut setzen.\n\u0026bull;Die ger√§te√ºbergreifende Nutzeranalyse k√∂nnen Sie in Ihrem Google-Account unter ‚ÄûMeine Daten  pers√∂nliche Daten‚Äú deaktivieren.\n\r\r\rSocial-Media-Plug-ins\rWir setzen auf unserer Webseite Social-Media-Plug-ins von sozialen Netzwerken ein. Dabei nutzen wir die sog.\u0026nbsp;\u0026bdquo;Zwei-Klick-L\u0026ouml;sung\u0026ldquo;-Shariff\u0026nbsp;von c\u0026rsquo;t bzw. heise.de: https://www.heise.de/ct/artikel/Shariff-Social-Media-Buttons-mit-Datenschutz-2467514.html; Dienstanbieter: Heise Medien GmbH \u0026amp; Co. KG, Karl-Wiechert-Allee 10, 30625 Hannover, Deutschland; Datenschutzerkl\u0026auml;rung: https://www.heise.de/Datenschutzerklaerung-der-Heise-Medien-GmbH-Co-KG-4860.html.\n\rDatenkategorie und Beschreibung der Datenverarbeitung: Nutzungsdaten, Inhaltsdaten, Bestandsdaten. Beim Abruf unserer Website werden durch \u0026bdquo;Shariff\u0026ldquo;\u0026nbsp;keine personenbezogenen Daten\u0026nbsp;an die Drittanbieter der Social-Plug-ins \u0026uuml;bermittelt. Neben dem Logo bzw. der Marke des sozialen Netzwerks finden Sie einen Regler, mit dem Sie das Plug-in per Klick aktivieren k\u0026ouml;nnen. Diese Aktivierung stellt Ihre Einwilligung in der Form dar, dass der jeweilige Anbieter des sozialen Netzwerks die Information erh\u0026auml;lt, dass Sie unsere Website aufgerufen haben und Ihre personenbezogenen Daten an den Anbieter des Plug-ins \u0026uuml;bermittelt und dort gespeichert werden. Hierbei handelt es sich um sog. Thirdparty Cookies. Bei einigen Anbietern wie Facebook und XING wird nach deren Angaben Ihre IP nach der Erhebung sofort anonymisiert. Die \u0026uuml;ber den Nutzer erhobenen Daten speichert der Plug-in-Anbieter als Nutzungsprofile. Sie k\u0026ouml;nnen Ihre Einwilligung jederzeit durch die Deaktivierung des Reglers widerrufen.\n\rZweck der Datenverarbeitung: Verbesserung und Optimierung unserer Website; Steigerung unserer Bekanntheit mittels sozialer Netzwerke; M√∂glichkeit der Interaktion mit Ihnen und der Nutzer untereinander √ºber soziale Netzwerke; Werbung, Analyse und/oder bedarfsgerechten Gestaltung der Website.\n\rRechtsgrundlagen: Die Rechtsgrundlage f√ºr die Verarbeitung der personenbezogenen Daten ist unser in den obigen Zwecken liegendes berechtigtes Interesse gem√§√ü Art. 6 Abs. 1 S. 1 lit. f) DS-GVO. Soweit Sie uns bzw. dem Verantwortlichen des sozialen Netzwerks eine Einwilligung in die Verarbeitung Ihrer personenbezogenen Daten erteilt haben, ist Rechtsgrundlage Art. 6 Abs. 1 S. 1 lit. a) i.V.m. Art. 7 DS-GVO. Bei vorvertraglichen Anfragen oder bei der Nutzung Ihrer personenbezogenen Daten zur Vertragserf√ºllung, ist Art. 6 Abs. 1 S. 1 lit. b) DS-GVO Rechtsgrundlage.\n\rDaten√ºbermittlung/Empf√§ngerkategorie: Soziales Netzwerk; soweit die US-Anbieter unter dem Privacy-Shield-Abkommen zertifiziert (https://www.privacyshield.gov/EU-US-Framework) sind, wird sichergestellt, dass das europ√§ische Datenschutzrecht eingehalten wird.\n\rGenutzte soziale Netzwerke und Widerspruch: Wir verweisen hinsichtlich des Zwecks und Umfangs der Datenerhebung und Verarbeitung auf die jeweiligen Datenschutzerkl√§rungen der sozialen Netzwerke. Zudem finden Sie dort auch Hinweise zu Ihren Rechten und Einstellungsm√∂glichkeiten zum Schutz Ihrer personenbezogenen Daten. Ihnen steht ein Widerspruchsrecht gegen die Bildung dieser Nutzerprofile zu, wobei Sie sich zur Aus√ºbung dieser Rechte direkt an den jeweiligen Plug-in-Anbieter wenden k√∂nnen.\n\r\rFacebook\nWir haben auf unserer Website Plug-ins vom sozialen Netzwerk Facebook.com (Firmensitz in der EU: Facebook Ireland Ltd., 4 Grand Canal Square, Grand Canal Harbour, Dublin 2, Irland) im Rahmen der sog. ‚ÄûZwei-Klick-L√∂sung‚Äú von Shariff integriert.Diese erkennen Sie am Facebook-Logo ‚Äûf‚Äú bzw. dem Zusatz ‚ÄûLike‚Äú, ‚ÄûGef√§llt mir‚Äú oder ‚ÄûShare‚Äú.\n\rSobald Sie willentlich das Facebook-Plug-in aktivieren, wird hierbei eine Verbindung von Ihrem Browser zu den Servern von Facebook hergestellt. Dabei erh√§lt Facebook die Information, einschlie√ülich Ihrer IP, dass Sie unsere Website aufgerufen haben und √ºbertr√§gt diese Information an Server von Facebook in den USA, wo diese Information gespeichert wird. Wenn Sie bei Facebook in Ihren Account eingeloggt sind, kann Facebook diese Information Ihrem Account zuordnen. Bei Nutzung der Funktionen des Plug-ins, z.B. Bet√§tigung des ‚ÄûLike‚Äú-Buttons, werden diese Informationen ebenfalls von Ihrem Browser an die Server von Facebook in den USA √ºbertragen und dort gespeichert sowie in Ihrem Facebook-Profil und ggf. bei Ihren Freunden angezeigt.\n\rZweck und Umfang der Datenerhebung sowie ihre weitere Verarbeitung und Nutzung der Daten durch Facebook sowie Ihre diesbez\u0026uuml;glichen Rechte und Einstellungsm\u0026ouml;glichkeiten zum Schutz Ihrer Privatsph\u0026auml;re, k\u0026ouml;nnen Sie den Datenschutzhinweisen von Facebook entnehmen:\u0026nbsp;https://www.facebook.com/about/privacy/. Datenerhebung beim \u0026bdquo;Gef\u0026auml;llt mir\u0026ldquo;-Button:\u0026nbsp;https://www.facebook.com/help/186325668085084. Ihre Einstellungen hinsichtlich der Nutzung Ihrer Profildaten zu Werbezwecken bei Facebook k\u0026ouml;nnen Sie hier verwalten und widersprechen:\u0026nbsp;https://www.facebook.com/ads/preferences/.\n\rWenn Sie sich bei Facebook vor dem Besuch unserer Website ausloggen und Ihre Cookies l√∂schen, werden bei der Aktivierung des Plug-ins keine Daten √ºber den Besuch unserer Website Ihrem Profil auf Facebook zugeordnet.\n\rFacebook hat sich dem Privacy Shield unterworfen und stellt damit sicher, dass europ\u0026auml;isches Datenschutzrecht eingehalten wird:\u0026nbsp;https://www.privacyshield.gov/EU-US-Framework.\n\rVereinbarung \u0026uuml;ber gemeinsame Verarbeitung personenbezogener Daten auf Facebook-Seiten (Art. 26 DS-GVO): https://www.facebook.com/legal/terms/page_controller_addendum, Datenschutzhinweise f\u0026uuml;r Facebook-Seiten: https://www.facebook.com/legal/terms/information_about_page_insights_data.\n\r\rTwitter\nWir haben auf unserer Website Plug-Ins des sozialen Netzwerks Twitter.com (Twitter Inc., 1355 Market St., Suite 900, San Francisco, California 94103, USA) im Rahmen der sog. \u0026bdquo;Zwei-Klick-L\u0026ouml;sung\u0026ldquo; von Shariff integriert. Diese Plug-Ins erkennen Sie an dem Twitter-Logo mit wei\u0026szlig;em Vogel auf blauem Hintergrund. Eine \u0026Uuml;bersicht \u0026uuml;ber Twitter-Buttons bzw. Tweets finden Sie unter:\u0026nbsp;https://developer.twitter.com/en/docs/twitter-for-websites/overview.\n\rWenn Sie in Ihren Twitter-Account eingeloggt sind, w√§hrend Sie die Twitter-Plug-ins willentlich aktivieren, kann Twitter den Anruf unserer Website Ihrem Twitter-Profil zuordnen. \rWenn Sie die Daten√ºbermittlung bei Aktivierung des Plug-ins an Twitter ausschlie√üen m√∂chten, dann loggen Sie sich vor dem Besuch unserer Website bei Twitter aus und l√∂schen Ihre Cookies.\n\rZweck und Umfang der Datenerhebung sowie ihre weitere Verarbeitung und Nutzung der Daten durch Twitter sowie Ihre diesbez\u0026uuml;glichen Rechte und Einstellungsm\u0026ouml;glichkeiten zum Schutz Ihrer Privatsph\u0026auml;re, k\u0026ouml;nnen Sie den Datenschutzhinweisen von Twitter entnehmen:\u0026nbsp;https://twitter.com/de/privacy. Widerspruch (Opt-Out):\u0026nbsp;https://twitter.com/personalization.\n\rTwitter hat sich dem Privacy Shield unterworfen und stellt damit sicher, dass europ\u0026auml;isches Datenschutzrecht eingehalten wird:\u0026nbsp;https://www.privacyshield.gov/EU-US-Framework.\n\r\rInstagram\nWir haben auf unserer Website Plug-Ins vom sozialen Netzwerk Instagram (Diensteanbieter: Facebook Ireland Ltd., 4 Grand Canal Square, Grand Canal Harbour, Dublin 2, Irland) im Rahmen der sog. ‚ÄûZwei-Klick-L√∂sung‚Äú von Shariff integriert. Diese erkennen Sie am Instagram-Logo in der Form einer viereckigen Kamera.\n\rWenn Sie willentlich das Plug-in aktivieren, wird hierbei eine Verbindung von Ihrem Browser zu den Servern von Instagram hergestellt. Dabei erh√§lt Instagram die Information, einschlie√ülich Ihrer IP-Adresse, dass Sie unsere Seite besucht haben und √ºbertr√§gt die Information an Server von Instagram in den USA, wo diese Information gespeichert wird. Wenn Sie bei Instagram in Ihren Account eingeloggt sind, kann Instagram diese Information Ihrem Account zuordnen und Sie k√∂nnen den Instagram-Button anklicken und so die Inhalte unserer Seiten auf Ihrem Instagram-Account teilen und speichern sowie ggf. Ihren dortigen Freunden anzeigen. Wir haben keine Kenntnis √ºber den genauen Inhalt der √ºbermittelten Daten, deren Nutzung und Speicherdauer durch Instagram.\n\rWenn Sie sich bei Instagram vor dem Besuch unserer Website ausloggen und Ihre Cookies l√∂schen, werden bei der Aktivierung des Plug-ins keine Daten √ºber den Besuch unserer Website Ihrem Profil auf Instagram zugeordnet.\n\rSie erhalten weitere Informationen in der Datenschutzerkl√§rung/ Opt-Out von Instagram unter¬†/ Opt-Out: https://help.instagram.com/519522125107875, Widerspruch: https://help.instagram.com/contact/186020218683230; Privacy Shield: https://www.privacyshield.gov/participant?id=a2zt0000000GnywAAC\u0026status=Active; Vereinbarung √ºber gemeinsame Verarbeitung personenbezogener Daten auf Instagram-Seiten (Art. 26 DS-GVO): https://www.facebook.com/legal/terms/page_controller_addendum.\n\r\r\rRechte der betroffenen Person\rWiderspruch oder Widerruf gegen die Verarbeitung Ihrer Daten\nSoweit die Verarbeitung auf Ihrer Einwilligung gem√§√ü Art. 6 Abs. 1 S. 1 lit. a), Art. 7 DS-GVO beruht, haben Sie das Recht, die Einwilligung jederzeit zu widerrufen. Die Rechtm√§√üigkeit der aufgrund der Einwilligung bis zum Widerruf erfolgten Verarbeitung wird dadurch nicht ber√ºhrt.\nSoweit wir die Verarbeitung Ihrer personenbezogenen Daten auf die Interessenabw√§gung gem√§√ü Art. 6 Abs. 1 S. 1 lit. f) DS-GVO st√ºtzen, k√∂nnen Sie Widerspruch gegen die Verarbeitung einlegen. Dies ist der Fall, wenn die Verarbeitung insbesondere nicht zur Erf√ºllung eines Vertrags mit Ihnen erforderlich ist, was von uns jeweils bei der nachfolgenden Beschreibung der Funktionen dargestellt wird. Bei Aus√ºbung eines solchen Widerspruchs bitten wir um Darlegung der Gr√ºnde, weshalb wir Ihre personenbezogenen Daten nicht wie von uns durchgef√ºhrt verarbeiten sollten. Im Falle Ihres begr√ºndeten Widerspruchs pr√ºfen wir die Sachlage und werden entweder die Datenverarbeitung einstellen bzw. anpassen oder Ihnen unsere zwingenden schutzw√ºrdigen Gr√ºnde aufzeigen, aufgrund derer wir die Verarbeitung fortf√ºhren.\nSie k√∂nnen der Verarbeitung Ihrer personenbezogenen Daten f√ºr Zwecke der Werbung und Datenanalyse jederzeit widersprechen. Das Widerspruchsrecht k√∂nnen Sie kostenfrei aus√ºben. √úber Ihren Werbewiderspruch k√∂nnen Sie uns unter folgenden Kontaktdaten informieren:\nJonathan Ratschat\nKronberger Stra√üe 43\n60323, Frankfurt am Main, Deutschland\nE-Mail-Adresse: j[point]ratschat[at]protonmail[point]com\n \nRecht auf Auskunft\nSie haben ein Recht auf Auskunft √ºber Ihre bei uns gespeicherten pers√∂nlichen Daten nach Art. 15 DS-GVO. Dies beinhaltet insbesondere die Auskunft √ºber die Verarbeitungszwecke, die Kategorie der personenbezogenen Daten, die Kategorien von Empf√§ngern, gegen√ºber denen Ihre Daten offengelegt wurden oder werden, die geplante Speicherdauer, die Herkunft ihrer Daten, sofern diese nicht direkt bei Ihnen erhoben wurden.\n\rRecht auf Berichtigung\nSie haben ein Recht auf Berichtigung unrichtiger oder auf Vervollst√§ndigung richtiger Daten nach Art. 16 DS-GVO.\r\rRecht auf L√∂schung\nSie haben ein Recht auf L√∂schung Ihrer bei uns gespeicherten Daten nach Art. 17 DS-GVO, es sei denn gesetzliche oder vertraglichen Aufbewahrungsfristen oder andere gesetzliche Pflichten bzw. Rechte zur weiteren Speicherung stehen dieser entgegen.\r\rRecht auf Einschr√§nkung\nSie haben das Recht, eine Einschr√§nkung bei der Verarbeitung Ihrer personenbezogenen Daten zu verlangen, wenn eine der Voraussetzungen in Art. 18 Abs. 1 lit. a) bis d) DS-GVO erf√ºllt ist:\n‚Ä¢ Wenn Sie die Richtigkeit der Sie betreffenden personenbezogenen f√ºr eine Dauer bestreiten, die es dem Verantwortlichen erm√∂glicht, die Richtigkeit der personenbezogenen Daten zu √ºberpr√ºfen;\n‚Ä¢ die Verarbeitung unrechtm√§√üig ist und Sie die L√∂schung der personenbezogenen Daten ablehnen und stattdessen die Einschr√§nkung der Nutzung der personenbezogenen Daten verlangen;\n‚Ä¢ der Verantwortliche die personenbezogenen Daten f√ºr die Zwecke der Verarbeitung nicht l√§nger ben√∂tigt, Sie diese jedoch zur Geltendmachung, Aus√ºbung oder Verteidigung von Rechtsanspr√ºchen ben√∂tigen, oder\n‚Ä¢ wenn Sie Widerspruch gegen die Verarbeitung gem√§√ü Art. 21 Abs. 1 DS-GVO eingelegt haben und noch nicht feststeht, ob die berechtigten Gr√ºnde des Verantwortlichen gegen√ºber Ihren Gr√ºnden √ºberwiegen.\n\rRecht auf Daten√ºbertragbarkeit\nSie haben ein Recht auf Daten√ºbertragbarkeit nach Art. 20 DS-GVO, was bedeutet, dass Sie die bei uns √ºber Sie gespeicherten personenbezogenen Daten in einem strukturierten, g√§ngigen und maschinenlesbaren Format erhalten k√∂nnen oder die √úbermittlung an einen anderen Verantwortlichen verlangen k√∂nnen.\r\rRecht auf Beschwerde\nSie haben ein Recht auf Beschwerde bei einer Aufsichtsbeh√∂rde. In der Regel k√∂nnen Sie sich hierf√ºr an die Aufsichtsbeh√∂rde insbesondere in dem Mitgliedstaat ihres Aufenthaltsorts, ihres Arbeitsplatzes oder des Orts des mutma√ülichen Versto√ües wenden.\r\r\r\rDatensicherheit\rUm alle personenbezogen Daten, die an uns √ºbermittelt werden, zu sch√ºtzen und um sicherzustellen, dass die Datenschutzvorschriften von uns, aber auch unseren externen Dienstleistern eingehalten werden, haben wir geeignete technische und organisatorische Sicherheitsma√ünahmen getroffen. Deshalb werden unter anderem alle Daten zwischen Ihrem Browser und unserem Server √ºber eine sichere SSL-Verbindung verschl√ºsselt √ºbertragen.\n\rStand: 07.06.2020\rQuelle: Muster-Datenschutzerkl√§rung von JuraForum.de\n","date":1588287600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1588287600,"objectID":"18d05a63a1c8d7ed973cc51838494e41","permalink":"/privacy/","publishdate":"2020-05-01T00:00:00+01:00","relpermalink":"/privacy/","section":"","summary":"Wir informieren Sie nachfolgend gem√§√ü den gesetzlichen Vorgaben des Datenschutzrechts (insb. gem√§√ü BDSG n.F. und der europ√§ischen Datenschutz-Grundverordnung ‚ÄöDS-GVO‚Äò) √ºber die Art, den Umfang und Zweck der Verarbeitung personenbezogener Daten durch unser Unternehmen.","tags":null,"title":"Datenschutz","type":"page"},{"authors":null,"categories":["Data Science","Research"],"content":"About the paper This research paper was written in summer 2020 within the Master\u0026rsquo;s seminar Data Mining in Marketing: Data Driven Customer Analytics with Machine Learning. In partial fulfillment of the requirements of the seminar, I predicted and interpreted cross-selling purchase probabilities using XGBoost and SHAP values in R. Read the full paper here: Boosting Gradient Boosting Interpretability: Predicting and Interpreting Cross-Selling Purchase Probabilities of a Large German Savings Bank.\nIntroduction and Findings In this paper, a dataset from a large German savings bank is used to predict crossselling purchase probabilities and decisions in the customer base. The goals of this paper are (1) to accurately predict whether an already existing customer will open a checking account and (2) to explore which effect the features have on the prediction to enhance the interpretability of the model. To reach these goals, the paper leverages one of the leading gradient boosting algorithms there is, namely XGBoost. It has been used with great success on many machine learning and data mining challenges. Among the advantages of XGBoost are that its models are easily scalable, tend to avoid overfitting, and can be used for a wide range of problems (Chen \u0026amp; Guestrin 2016, p. 785-786). To tackle the lack of easy interpretability of boosted trees (Friedman 2001, p. 1229‚Äì1230), this paper implements SHapley Additive exPlanations (SHAP) values to explain the output of the XGBoost model.\nWhen accuracy is the main goals, then one should use more complex models. Although tuning an XGBoost model is computationally expensive, once it is tuned, it becomes computationally cheap. It becomes apparent that such prediction systems have their place in marketing analytics departments. Instead of only predicting the likelihood of buying a checking account, marketing departments could extend this approach to any product offering to compare purchasing probabilities and target customers with the products for which they have the highest probability to buy. Through this strategy, it could become feasible to target the right customers and to ultimately increase profitability. For this concept to be successful, several additional topics need to be addressed. One has to assess which customers have the potential to be profitable while excluding customers that lead to losses (Shah et al. 2012). Also, one must answer the question when and where a customer should be targeted.\nGenerally, SHAP values enable its users to critically examine complex models and to understand how dependent variables were predicted. Through this method, users gain further knowledge about importance, extent and direction of feature variables on the target variable. Although causal statements cannot be made through this approach, it still helps users to gain trust in the model, to find ways of improving the model, and to get a new understanding of the data. Therefore, this enhanced interpretability should increase user adoption. When only using tools from the xgboost package, this would not be feasible to such an extent. The trust of the analysis through SHAP values is increased because suggested underlying trends of the data have been amongst others discovered by RFM-models (Bauer 1988, Miglautsch 2000). Customers that have recently acquired another product have an higher prediction value than customers that have not bought another product for a longer period of time. Also, the more active customers are (as measured by logins), the higher is the prediction that these customers cross-buy a checking account. Other important trends found in the data are that younger customers exhibit an higher prediction probability than older customers and that checking account ads always lead to a positive effect on the prediction, although with varying extent. This paper shows that giro_mailing does lead to a negative interaction effect on the prediction when appearing with younger or recent customers. This could indicate that the model finds autoresponse within the group of younger or recent customers and punishes this effect with a negative interaction value.\nXGBoost models should be used in practice for predicting cross-selling purchase probabilities and decisions. One of the biggest disadvantages - lack of interpretability - can be mitigated through the use of SHAP values that greatly expand the transparency, explainability, interpretability of complex tree-based models.\n","date":1580083200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1580083200,"objectID":"13348f02b61a1a4918c3b660ce523ae5","permalink":"/project/boosting-gradient-boosting-interpretability/","publishdate":"2020-01-27T00:00:00Z","relpermalink":"/project/boosting-gradient-boosting-interpretability/","section":"project","summary":"I predicted and interpreted cross-selling purchase probabilities using XGBoost and SHAP values in R.","tags":["Research","Data Science"],"title":"Boosting Gradient Boosting Interpretability","type":"project"},{"authors":null,"categories":["Research"],"content":"About the paper This research paper was written in winter 2019/20 within the Master\u0026rsquo;s seminar Big Data in Personal Finance. In partial fulfillment of the requirements of the lecture, I analyzed the Survey of Consumer Finances 2016 with regards to the effect of financial literacy to stock market participation using R. Read the full paper here: Financial Literacy and Stock Market Participation: Evidence From the Survey of Consumer Finances.\nIntroduction and Findings The stockholding puzzle, pioneered in the research of Haliassos and Bertaut (1995), has gained a lot of attention by researchers. The question is why a large part of the population worldwide does not invest in stocks despite the equity premium and prefers to hold their assets in low-rate liquid assets (Haliassos \u0026amp; Bertaut, 1995, p. 1110). Due to the demographic shift in the population, this question has become more important in the last years. Stocks and their long-term wealth generating potential can help retirees to not be dependent on the struggling social security and pension systems (Christelis, Georgarakos, \u0026amp; Haliassos, 2011, p. 1918).\nThe contribution to the existing literature is that this paper analyzes the Survey of Consumer Finances (SCF) 2016 with regards to the effect of financial literacy to stock market participation. A new set of financial literacy questions was included in the SCF 2016 that makes it feasible to get an objective measure of financial literacy. This was not possible in earlier series of the SCF which resulted in financial literacy either not being included in the analysis [see, e.g. (Campell, 2006); (Haliassos \u0026amp; Bertaut, 1995); (Malmendier \u0026amp; Nagel, 2011)] or being included through proxies [see, e.g. (Christelis et al., 2011); (Huston et al., 2012); (Shum \u0026amp; Faig, 2006)]. While controlling for other effects, the measure of financial literacy used in this paper enables a more precise estimation of its effect on stock market participation.\nIn the descriptive analysis, I was able to show that there is a difference of stock market participation conditional on financial literacy. Also, financial literacy proxies used in previous research like education and wealth show structural differences in stock market participation when separated in financial literacy groups. More importantly, the Probit regression analysis confirms the results that financial literacy has indeed a significant effect in the decision to participate in the stock market. Therefore, I show that a lack of financial knowledge is a significant deterrent to stock ownership.\n","date":1580083200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1580083200,"objectID":"a0e96dceeae0c33c51306490885990bb","permalink":"/project/financial-literacy/","publishdate":"2020-01-27T00:00:00Z","relpermalink":"/project/financial-literacy/","section":"project","summary":"I analyzed the Survey of Consumer Finances 2016 with regards to the effect of financial literacy to stock market participation using R.","tags":["Research"],"title":"Financial Literacy and Stock Market Participation","type":"project"},{"authors":null,"categories":null,"content":"Impressum\rJonathan Ratschat\nKronberger Stra√üe 43\n60323 Frankfurt am Main\nTelefon: +4915236279713\nE-Mail: j[point]ratschat[at]protonmail[point]com\nVerantwortlich f√ºr den Inhalt (gem. ¬ß 55 Abs. 2 RStV):\nJonathan Ratschat\nKronberger Stra√üe 43\n60323 Frankfurt am Main\nHinweis gem\u0026auml;\u0026szlig; Online-Streitbeilegungs-VerordnungNach geltendem Recht sind wir verpflichtet, Verbraucher auf die Existenz der Europ\u0026auml;ischen Online-Streitbeilegungs-Plattform hinzuweisen, die f\u0026uuml;r die Beilegung von Streitigkeiten genutzt werden kann, ohne dass ein Gericht eingeschaltet werden muss. F\u0026uuml;r die Einrichtung der Plattform ist die Europ\u0026auml;ische Kommission zust\u0026auml;ndig. Die Europ\u0026auml;ische Online-Streitbeilegungs-Plattform ist hier zu finden: http://ec.europa.eu/odr. Unsere E-Mail lautet: j[point]ratschat[at]protonmail[point]com\nWir weisen aber darauf hin, dass wir nicht bereit sind, uns am Streitbeilegungsverfahren im Rahmen der Europ\u0026auml;ischen Online-Streitbeilegungs-Plattform zu beteiligen. Nutzen Sie zur Kontaktaufnahme bitte unsere obige E-Mail und Telefonnummer.\nDisclaimer ‚Äì rechtliche Hinweise\r¬ß 1 Warnhinweis zu Inhalten\nDie kostenlosen und frei zug√§nglichen Inhalte dieser Webseite wurden mit gr√∂√ütm√∂glicher Sorgfalt erstellt. Der Anbieter dieser Webseite √ºbernimmt jedoch keine Gew√§hr f√ºr die Richtigkeit und Aktualit√§t der bereitgestellten kostenlosen und frei zug√§nglichen journalistischen Ratgeber und Nachrichten. Namentlich gekennzeichnete Beitr√§ge geben die Meinung des jeweiligen Autors und nicht immer die Meinung des Anbieters wieder. Allein durch den Aufruf der kostenlosen und frei zug√§nglichen Inhalte kommt keinerlei Vertragsverh√§ltnis zwischen dem Nutzer und dem Anbieter zustande, insoweit fehlt es am Rechtsbindungswillen des Anbieters.\n¬ß 2 Externe Links\nDiese Website enth√§lt Verkn√ºpfungen zu Websites Dritter (\"externe Links\"). Diese Websites unterliegen der Haftung der jeweiligen Betreiber. Der Anbieter hat bei der erstmaligen Verkn√ºpfung der externen Links die fremden Inhalte daraufhin √ºberpr√ºft, ob etwaige Rechtsverst√∂√üe bestehen. Zu dem Zeitpunkt waren keine Rechtsverst√∂√üe ersichtlich. Der Anbieter hat keinerlei Einfluss auf die aktuelle und zuk√ºnftige Gestaltung und auf die Inhalte der verkn√ºpften Seiten. Das Setzen von externen Links bedeutet nicht, dass sich der Anbieter die hinter dem Verweis oder Link liegenden Inhalte zu Eigen macht. Eine st√§ndige Kontrolle der externen Links ist f√ºr den Anbieter ohne konkrete Hinweise auf Rechtsverst√∂√üe nicht zumutbar. Bei Kenntnis von Rechtsverst√∂√üen werden jedoch derartige externe Links unverz√ºglich gel√∂scht.\n¬ß 3 Urheber- und Leistungsschutzrechte\nDie auf dieser Website ver√∂ffentlichten Inhalte unterliegen dem deutschen Urheber- und Leistungsschutzrecht. Jede vom deutschen Urheber- und Leistungsschutzrecht nicht zugelassene Verwertung bedarf der vorherigen schriftlichen Zustimmung des Anbieters oder jeweiligen Rechteinhabers. Dies gilt insbesondere f√ºr Vervielf√§ltigung, Bearbeitung, √úbersetzung, Einspeicherung, Verarbeitung bzw. Wiedergabe von Inhalten in Datenbanken oder anderen elektronischen Medien und Systemen. Inhalte und Rechte Dritter sind dabei als solche gekennzeichnet. Die unerlaubte Vervielf√§ltigung oder Weitergabe einzelner Inhalte oder kompletter Seiten ist nicht gestattet und strafbar. Lediglich die Herstellung von Kopien und Downloads f√ºr den pers√∂nlichen, privaten und nicht kommerziellen Gebrauch ist erlaubt.\nDie Darstellung dieser Website in fremden Frames ist nur mit schriftlicher Erlaubnis zul√§ssig.\n¬ß 4 Besondere Nutzungsbedingungen\nSoweit besondere Bedingungen f√ºr einzelne Nutzungen dieser Website von den vorgenannten Paragraphen abweichen, wird an entsprechender Stelle ausdr√ºcklich darauf hingewiesen. In diesem Falle gelten im jeweiligen Einzelfall die besonderen Nutzungsbedingungen.Quelle: Impressum Vorlage von JuraForum.de\n","date":1530140400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1530140400,"objectID":"9b10c1f64082d3869fd4cb1f85809430","permalink":"/terms/","publishdate":"2018-06-28T00:00:00+01:00","relpermalink":"/terms/","section":"","summary":"Impressum\rJonathan Ratschat\nKronberger Stra√üe 43\n60323 Frankfurt am Main\nTelefon: +4915236279713\nE-Mail: j[point]ratschat[at]protonmail[point]com\nVerantwortlich f√ºr den Inhalt (gem. ¬ß 55 Abs. 2 RStV):\nJonathan Ratschat\nKronberger Stra√üe 43\n60323 Frankfurt am Main","tags":null,"title":"Impressum","type":"page"},{"authors":null,"categories":["Student Initiative"],"content":"","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"9e28a33e3b5b6cf842be5329f922029c","permalink":"/project/techacademy/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/techacademy/","section":"project","summary":"TechAcademy e.V. prepares students for the digital future in the fields of Data Science and Web Development. Have a look on our website.","tags":["Student Initiative"],"title":"TechAcademy","type":"project"}]