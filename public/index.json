[{"authors":["admin"],"categories":null,"content":"Luckily, I get to work with a lot of data. Applying econometric or machine learning methods are fun. Why? Because I like to code. Also, having exciting results once I have finished a project is satisfying.\nIn my spare time, I am active at TechAcademy. This student initiative prepares students for the digital future in the fields of Data Science and Web Development. My role here is to create partnerships with companies. Lots of work, but it enables me to meet inspiring people. Also, pitching (aka selling) is a skill that is built through experience - not through lectures.\nAlso, I like to write down my experiences and learnings every now and then. Keeping it to myself would be selfish üòú Therefore, I am sharing it here üöÄ\n","date":1554595200,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1554595200,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/author/jonathan-ratschat/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/jonathan-ratschat/","section":"authors","summary":"Luckily, I get to work with a lot of data. Applying econometric or machine learning methods are fun. Why? Because I like to code. Also, having exciting results once I have finished a project is satisfying.","tags":null,"title":"Jonathan Ratschat","type":"authors"},{"authors":["Âê≥ÊÅ©ÈÅî"],"categories":null,"content":"Âê≥ÊÅ©ÈÅî is a professor of artificial intelligence at the Stanford AI Lab. His research interests include distributed robotics, mobile computing and programmable matter. He leads the Robotic Neurobiology group, which develops self-reconfiguring robots, systems of self-organizing robots, and mobile sensor networks.\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Sed neque elit, tristique placerat feugiat ac, facilisis vitae arcu. Proin eget egestas augue. Praesent ut sem nec arcu pellentesque aliquet. Duis dapibus diam vel metus tempus vulputate.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"d8f258c323db746988131e8c2d192f9a","permalink":"/author/%E5%90%B3%E6%81%A9%E9%81%94/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/%E5%90%B3%E6%81%A9%E9%81%94/","section":"authors","summary":"Âê≥ÊÅ©ÈÅî is a professor of artificial intelligence at the Stanford AI Lab. His research interests include distributed robotics, mobile computing and programmable matter. He leads the Robotic Neurobiology group, which develops self-reconfiguring robots, systems of self-organizing robots, and mobile sensor networks.","tags":null,"title":"Âê≥ÊÅ©ÈÅî","type":"authors"},{"authors":null,"categories":null,"content":"Flexibility This feature can be used for publishing content such as:\n Online courses Project or software documentation Tutorials  The courses folder may be renamed. For example, we can rename it to docs for software/project documentation or tutorials for creating an online course.\nDelete tutorials To remove these pages, delete the courses folder and see below to delete the associated menu link.\nUpdate site menu After renaming or deleting the courses folder, you may wish to update any [[main]] menu links to it by editing your menu configuration at config/_default/menus.toml.\nFor example, if you delete this folder, you can remove the following from your menu configuration:\n[[main]] name = \u0026quot;Courses\u0026quot; url = \u0026quot;courses/\u0026quot; weight = 50  Or, if you are creating a software documentation site, you can rename the courses folder to docs and update the associated Courses menu configuration to:\n[[main]] name = \u0026quot;Docs\u0026quot; url = \u0026quot;docs/\u0026quot; weight = 50  Update the docs menu If you use the docs layout, note that the name of the menu in the front matter should be in the form [menu.X] where X is the folder name. Hence, if you rename the courses/example/ folder, you should also rename the menu definitions in the front matter of files within courses/example/ from [menu.example] to [menu.\u0026lt;NewFolderName\u0026gt;].\n","date":1536451200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1536451200,"objectID":"59c3ce8e202293146a8a934d37a4070b","permalink":"/courses/example/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/courses/example/","section":"courses","summary":"Learn how to use Academic's docs layout for publishing online courses, software documentation, and tutorials.","tags":null,"title":"Overview","type":"docs"},{"authors":null,"categories":null,"content":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 2 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"74533bae41439377bd30f645c4677a27","permalink":"/courses/example/example1/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/example/example1/","section":"courses","summary":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":null,"title":"Example Page 1","type":"docs"},{"authors":null,"categories":null,"content":"Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 4 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"1c2b5a11257c768c90d5050637d77d6a","permalink":"/courses/example/example2/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/example/example2/","section":"courses","summary":"Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":null,"title":"Example Page 2","type":"docs"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature.   Slides can be added in a few ways:\n Create slides using Academic\u0026rsquo;s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes.  Further talk details can easily be added to this page using Markdown and $\\rm \\LaTeX$ math code.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"96344c08df50a1b693cc40432115cbe3","permalink":"/talk/example/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/example/","section":"talk","summary":"An example talk using Academic's Markdown slides feature.","tags":[],"title":"Example Talk","type":"talk"},{"authors":null,"categories":null,"content":"I was failing. I had worked my ass off. But somehow, I did not see any success.\nIt was not that I wasn\u0026rsquo;t reaching anyone. I got meetings with interested potential clients. But after having had numerous pitches, I did not see any results.\nI got rejected. Often. I heard all the excuses.\n Sorry, we don\u0026rsquo;t have have any budget right now. Let\u0026rsquo;s talk in half a year. I talked with the relevant division. They don\u0026rsquo;t have time to organize something. Your proposal sounds interesting. I\u0026rsquo;ll forward you to another part of the company where you\u0026rsquo;ll fit better.  And of course, I never heard back from the company.\nAt that time, I decided to work harder. To approach more clients. To get more meetings. Somewhere I had heard that hard work would ultimately pay off as a sales guy. Success was waiting for me.\nBut of course, nothing changed. The system that I had built was running, but not in my favor.\nOne day I met with one of the founders for lunch. All of a sudden, he said to me, \u0026ldquo;Jonathan, it\u0026rsquo;s not working, right?\u0026rdquo;. I responded, \u0026ldquo;I\u0026rsquo;m doing everything I can, but I don\u0026rsquo;t know what\u0026rsquo;s going on\u0026rdquo;.\n What he then told me was one of the best advice I\u0026rsquo;d ever been given.\n \u0026ldquo;Jonathan, I can\u0026rsquo;t help you. But I bet there is someone who can help you. Why don\u0026rsquo;t you ask Justin? He is one of the best sales guys I know\u0026rdquo;.\nI was on fire. Justin, yeah, I knew this guy. Startup founder. Cool guy, approachable, and lots of experience.\nOverly motivated as I was, I not only reached out to him but also his sales head. One thing led to another, and I had two meetings scheduled.\nWhat happened? We talked a lot about my sales process. About every little detail that I had never thought of. After meeting both of them for 30 minutes each, I was able to point out the mistakes I had made. Not just the mistakes, but also the things I was not doing.\nWhat happened then? I still put in the hard work. But this time, everything changed. Instead of zero offers, I had three on the table. Since we only had one spot open, I had to cancel two potential partners.\n When looking back, these mistakes seem so obvious. But little did I know back then.\n I had not asked anyone for advice when I started my new role. Everyone seemed to be determined that I was the very best candidate for this role. Therefore, it never crossed my mind to ask for help.\nAlso, when I was not successful, I did not question my approach. I mean, I was approaching clients and meeting with them. It was just bad luck that I did not get the response that I wanted.\nIt took someone else to open my eyes. And when my eyes were open, I was able to seek the advice I needed. And when I sought the direction I needed, my actions changed for good.\nThis is how I seek advice. I just ask for it. When I have a problem that Google cannot solve, I think about who I can ask. Do I know anyone who could get me going in the right direction? If not, do I have a friend that knows a lot of people in this particular field?\nWhen I have identified someone that can genuinely help me, I sent a message. Straightforward. Where? Most often, they are acquaintances, so I either have their phone number, or I am connected on LinkedIn with them.\nI state my problem. What is important here is that I am specific about my problem. That I cannot solve it by myself. Also, I show respect by asking this particular person for advice. I trust this person to be able to help me.\nWhat I ask for is a phone call. Most often, they are glad to help. I mean, who doesn\u0026rsquo;t like to help someone who is actively seeking one\u0026rsquo;s advice? Someone who shows respect and trust?\nI know it takes courage to ask an acquaintance or even a stranger for help. But ultimately, their input can help me to solve problems that I could not have solved by myself.\nWhat I\u0026rsquo;ve learned through this experience is that hard work is not always the solution. That I\u0026rsquo;m not an expert just because my job description says so. Therefore, I am allowed to get advice. And this advice can help me to get to the very next level.\nIf you have any questions or comments, feel free to reach me via the contact field or LinkedIn.\n","date":1591920000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1591920000,"objectID":"2ae4db4d8d50b23621ee100e60824812","permalink":"/post/an-expert-is-made-not-born/","publishdate":"2020-06-12T00:00:00Z","relpermalink":"/post/an-expert-is-made-not-born/","section":"post","summary":"Asking for advice is easy. It helped me when working harder was not the answer.","tags":["Personal Insights","Student Initiative","Sales"],"title":"An Expert is Made, Not Born","type":"post"},{"authors":null,"categories":null,"content":"This article was first published on Medium.\nHow great would it be when data could be as easily accessed as on Kaggle?\nData collection? Just visit Kaggle, find a suitable data set, and download it. How about analyzing the Titanic incident from 1912? Or how about image recognition of flowers? No? Maybe you want to predict credit card fraud detection? Guess what, Kaggle has you covered.\nWhen you have decided on your data set of interest, the fun can finally start. Taking 5 minutes to find a suitable data set was already stressful enough, right? The most elaborate machine algorithms are waiting for you. So ultimately, who cares about the data?\nSadly, the real world is different. Having the right data and data quality are key to making causal statements or to constructing machine learning algorithms that can really have an impact. Without relevant data, your analyses would be fun, but irrelevant.\nObviously, you cannot always find perfectly preprocessed data that fulfill your needs. Also, you need to understand where the data has come from and how it was built. Ultimately, we need to keep in mind what Kaggle is.\n It is most famous for being a place in which one can enter competitions to solve data science challenges.\n So if you want to make a name for yourself as a serious predictive modeler, you have found the perfect website to show off your skills. If you‚Äôre going to gather data to write a research paper or to build something that works in the real world, Kaggle could be the wrong source for your data.\nIn my experience, data collection and preparation can take days to complete. What I have done so far is to build data sets from scratch and access data sets from government institutions. Both have their limitations.\nWhat I want to show you is a practical introduction to how you could create relevant data sets that support you in your research/machine learning goals. Let‚Äôs get started.\nFirst, you have to assess the following two questions to conduct your analysis.\n What kind of data do you need? How can you access it?  Answering these questions is critical but not always straightforward. Of course, a Google search could lead to results, but asking peers for advice could also be helpful. Spend some time with these questions until you‚Äôre sure that you have found the right answer.\n1. Building a data set from scratch In one of my projects, I needed to access financial data from German companies to analyze the effect of a new mandatory accounting standard on bid-ask spreads.\nLuckily, my professor supplied us with a Thomson Reuters account, and I could use Datastream to access the financial data of these companies. You would think that simply using this database would be sufficient and that I could finally do the real work.\nFalse! When gathering the data for these companies, I ended up with 8 different excel sheets that I had to somehow merge into one data frame.\nDatastream provided me with some static company information that would end up as my main sheet.\n\rThe other excel sheets that I got had the following format because I was accessing time-series data for each company.\n\rSo how can I get such data into a meaningful format so that I can use it along with the other company information?\nLet‚Äôs perform one of my calculations so that you get the idea. I had two sheets ‚Äî one for bid prices and one for ask prices. What I needed was the average relative bid-ask spread.\nFirst, I loaded the data and controlled for missing values. I spotted one row that was completely missing and deleted it for both data sets.\n\rThen I calculated the bid-ask spread by subtracting the bid price from the ask price.\n\r'data.frame': 152 obs. of 50 variables:\r$ D.AB1 : num 0.034 0.069 0.038 ...\r$ D.AOX : num 0.38 0.36 0.38 ...\r$ D.AAD : num 0.38 0.4 0.36 ...\r$ D.CAG : num 0.04 0.1 0.04 ...\r$ D.B5A : num 0.36 0.395 0.395 ...\r$ D.BDT : num 0.37 0.75 1 ...\r$ D.BIO3: num 0.84 0.82 0.82 ...\r$ D.O2C : num 0.151 0.15 0.15 ...\r$ D.CEV : num 0.305 0.295 0.2 ...\r$ D.CWC : num 0.535 1.175 1.335 ...\r Then I had to calculate the relative bid-ask spread. Therefore, I had to import the daily stock prices, deleted the 149th row, and calculated the bid-ask spreads relative to the price.\n\rFinally, I calculated the mean of the relative bid-ask spreads and merged it into the static data frame.\n\r'data.frame': 50 obs. of 6 variables:\r$ MNEM : chr \u0026quot;D.2HRA\u0026quot; \u0026quot;D.AAD\u0026quot; \u0026quot;D.AB1\u0026quot; ...\r$ NAME : chr \u0026quot;H \u0026amp; R\u0026quot; \u0026quot;AMADEUS FIRE\u0026quot; ...\r$ WC05350 : POSIXct, format: \u0026quot;2011-12-31\u0026quot; ...\r$ MV : num 644 150 331 638 622 ...\r$ NOSHFF : num 44 74 63 44 52 45 100 ...\r$ mean_relative_bid_ask: num 0.0138 0.0139 0.0163 ...\r This is only the code for one additional variable! Imagine doing that for 20 or even 30 other variables that you cannot get out-of-the-box from Datastream. This takes way longer than 5 minutes.\nThere are many other feasible methods of how you can create your own data set from scratch. You could, for example, conduct a good old survey or scrape tweets from Twitter. Ultimately, it depends on what kind of data you need.\nOkay, it‚Äôs time for a quick assessment.\nAdvantages:\n Features are included based on the purpose of the research question or task. Not vice versa. This helps to only use meaningful data. It is traceable how the variables were created.  Disadvantages:\n It can be challenging to find suitable sources. It takes a lot of time to gather the data. Transforming features into the right format can be a lot of effort. Access to databases like Thomson Reuters is often restricted. If your university or employer does not have a license, this kind of information can get very costly.  2. Using a data set from a governmental institution One could think that accessing data from governmental institutions is as easy as obtaining data from Kaggle. Wrong! Often, you have to put in a lot of time to understand the data.\nSo for another project, I wanted to research the effect of financial literacy on stock market participation. For assessing this research question, I found quite a lot of research papers that made use of the Survey of Consumer Finances to analyze stock market participation. Therefore, I accessed the newest version (2016) of this cross-sectional survey of US families.\n Little did I know how difficult understanding and working with this data set would be.\n So the SEC has published all relevant data here. I spend a substantial time finding the right data set. First, I tried out the R-implementation, but in the end, I found it too difficult to use. Then, I accidentally downloaded the summary extract and wondered why the data was so different than described in the codebook. After a long journey of losing my mind, I found the complete data that I would be using for my analyses.\nWhen looking at the data, I found that all variables were encoded. I had to use this codebook to make sense of the data. For every single variable‚Ä¶\n\rThe codebook is really really long. Even the SEC notes on the first lines of the codebook the crazy size of this document. It contains about 45,000 lines of text. They recommend to not print the entire document. Great advice‚Ä¶\n\rFirst lines of the SCF‚Äôs Codebook.\r\r\rI spend many hours finding the questions that I could use to create meaningful variables. Ever used the search tool in Chrome? I probably used it 10,000 times for finding the right variables.\nLuckily, the summary extract that I first downloaded proofed to be helpful. Instead of calculating the numerous financial information of the households in R, I just merged this data set to the complete survey data set. Funny story: The calculations are specified in a SAS script. I attempted to translate it into R. After wasting two hours, I remembered the summary extract‚Ä¶\nFinally, I had a data set that I could start to analyze. But of course, I had many more problems with it. Have you ever heard of weighting and multiple imputations? These topics are difficult and painful, at least for me. But this is another story to tell.\nOf course, the SCF is only one of the numerous available governmental data sets. And besides governmental data, there are also data sets from organizations like the World Bank, WHO, or Unicef. Some might be much easier to handle.\nAgain, a quick assessment.\nAdvantages:\n Commonly high data quality. Especially if the data has been used by other researchers or practitioners. Often data is well documented. Therefore, one can understand how the variables were created.  Disadvantages:\n It can take a lot of time to gather and transform features (think about the 45,000 lines of text). It can be hard to understand the data sets. Access is sometimes only given on a request basis.  Conclusion So what have we learned? Accessing data can be quite a hassle and takes time. Loading data and being ready to rumble? No, this is unrealistic.\nBut what collecting data the old way (not on Kaggle) ultimately does is that we must ask ourselves the right questions. We have to think before we have the data. Why? Because collecting data is a lot of effort. This means that we hopefully collect and use meaningful data for our analyses.\n Keep in mind that an analysis can only be as good as the quality of the data.\n If you have any questions or comments, feel free to reach me via the contact field or LinkedIn.\n","date":1591401600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1591401600,"objectID":"07062caeecce7938b1e0cb61ae79bfd3","permalink":"/post/how-to-collect-data-for-your-analysis/","publishdate":"2020-06-06T00:00:00Z","relpermalink":"/post/how-to-collect-data-for-your-analysis/","section":"post","summary":"Hint: Using a Kaggle data set might not be sufficient.","tags":["Data Science","Research"],"title":"How to Collect Data for Your Analysis","type":"post"},{"authors":null,"categories":null,"content":"This article was first published on LinkedIn.\nMoney is essential as a student, and time is limited, right?\nBut what if I told you that joining a student initiative has been one of the best decisions that I have made in the last year? Please hear me out. I know that your schedule is more than full. But maybe, only maybe, it‚Äôs a decision that is also right for you.\nWhat I want to share is my journey with a so-called student initiative. Not any student initiative. A student initiative driven by a specific purpose and great people.\nThe first time I heard about TechAcademy was through a friend. At that time, my life was hectic, so I asked some questions and probably quickly switched the topic. ‚ÄúGood for him that he has the time to start a new student initiative without getting paid for it‚Äù I thought to myself. But since I was too focused on my own life, I did not hear him out.\nSome months passed, my life changed dramatically, and of a sudden, I saw an announcement on LinkedIn that this initiative was finally ready to take off. This time, I was more than interested. Why? Because I recognized it had a clear purpose. And this purpose related to me.\nSo what does TechAcademy do? TechAcademy prepares students for the digital future. Each semester, they offer 70 students from all faculties the opportunity to acquire programming skills in the fields of Data Science and Web Development.\nI had learned some lines of code during my first semester, but I‚Äôve never had the opportunity to apply it. Without having any projects or people that were using this programming language, I quickly stopped. But remembering that I once wanted to improve my skills, I could directly see that they were offering a valuable program.\nSo what did I do? I quickly reached out to one of the founders and offered my help. Why? I don‚Äôt know. Maybe I wanted to meet some new people, and perhaps I wanted to be part of something bigger than me. As I said, I don‚Äôt know.\nFast forward one and a half years. What has happened in this time is fantastic. I got to meet so many inspiring people, learned a lot professionally, and we, as a team, have been able to have an impact on the lives of over 190 students.\nHere are the three things that I got from joining a student initiative. 1. Community When working at my student initiative, I got to meet a lot of people.\nFirst, there is my team with whom I organize everything. I won‚Äôt lie. Being in a student initiative involves a lot of work. But through that work and the shared purpose, I got close to my team. We see each other every week, which led to new friendships. And these people are cool, driven, and inspiring ‚Äî precisely the people I want to call friends.\nSecond, I met so many participants. Each and everyone is different. At TechAcademy, we are open to every faculty. Why? Because each and everyone lives in a digital world that is exponentially changing.\nLuckily, we do not have just business students that participate in our program. I mean, when and where do I meet mathematics, physics, teaching, or humanities students all in one place? I find it interesting to get to know different perspectives.\nI gratefully remember one Ph.D. student in archaeology that joined our program. She wanted to learn Python to write an algorithm to classify ancient vases. How crazy is that?!\n2. Professional experience I have learned so much in my ‚Äújob‚Äù at TechAcademy. It feels like working in a small startup. I need to fulfill specific responsibilities. If I don‚Äôt do it, I let my team down. There‚Äôs nobody else to blame. Therefore, I learned to deliver.\nAlso, I have the freedom to bring in my ideas. I can make a difference. We are 15 people right now. Therefore, my voice gets heard. And on my team, there are only three of us. We can do our job as we please. Nobody tells us how we should do things.\nWe can try out new methods to approach potential partners. We can try out new negotiation techniques. In the end, we can assess what works and use it from that point on. Compare that to work in a big corporation as an intern. There you don‚Äôt question the status quo. There you just do what your colleagues ask you to do.\nGenerally, I have learned much more at TechAcademy than in any internship. As I said, you have to make stuff happen. My responsibility is to create partnerships with companies. Through these partnerships, we can offer our participants valuable insights on how digitalization is lived in practice. Also, we can finance our initiative through these partnerships since our program is entirely free of charge for students.\nSo why have I learned much more than in any other internship? I approach companies, have initial phone calls and meetings with them. I negotiate contracts, and I have to maintain the relationship once the contract is signed. Most importantly, I need to make sure that we, as a student initiative, solve a problem for the company. If not, they won‚Äôt work with us.\nI compare this with two sales-related internships that I had. There I prepared (potential) customer information and conducted market research. If I got fortunate, I was allowed to join a call or a meeting as a ‚Äúfly on the wall‚Äù. I mean, I am thankful for the experience and that I was paid, but these are indeed two different worlds.\n3. Caring for others by giving back I don‚Äôt remember which book it was (probably one of Tony Robbin‚Äôs books), but there, the author made clear that happiness can be achieved through helping others. That there is nothing more fulfilling than going the extra mile for another human being. To step back from one‚Äôs ego and to care about someone who‚Äôs not yourself.\nI have found truth in these words. Of course, being in a student initiative has had a significant impact on my own life. But when you think about it, I am sacrificing my time so that students can participate in the best-possible program that we can create.\nThe students are enabled to learn a new programming language. For many of them, it‚Äôs the first time that they have used a program other than Word or Excel. They don‚Äôt know what machine learning or data science is. But when they participate in our program, a whole new world has opened to them.\nAlso, they meet so many people who become good friends. Not everyone is good at meeting new people, but we make sure that they have the opportunity to get out of their comfort zone. One friend told me last semester:\n \u0026ldquo;It is so hard for me to meet new people. Now, after having been here every other week, I have met people with whom I go to the canteen or have study groups.\u0026rdquo;\n I can assure you that this has had a significant impact on her life. I bet that many other stories are similar.\nMaybe someone finds a new passion. Another one creates a living out of that passion. Maybe one person can finally write his or her empirical bachelor or master thesis without getting nightmares. Another one finds friends for life.\nThis, by itself, is very motivating. I am part of something that creates value in the lives of many people. It‚Äôs not about me, it‚Äôs about them. And this is something that I love about my work at TechAcademy.\nIndeed, joining TechAcademy had a meaningful impact on my life. I am thankful for everyone that made this journey possible, for all the people that I have met, and all the memorable days that I have spent with this initiative.\nThe journey goes on, and maybe, just maybe, I have inspired you to look for opportunities at your university or your neighborhood. Of course, TechAcademy is not the only cool initiative there is.\nIf you have any questions or comments, feel free to reach me via the contact field or LinkedIn.\n","date":1590451200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1590451200,"objectID":"e7fc2daeb8628b946fa539529f680efe","permalink":"/post/working-for-free-improved-my-life-in-3-great-ways/","publishdate":"2020-05-26T00:00:00Z","relpermalink":"/post/working-for-free-improved-my-life-in-3-great-ways/","section":"post","summary":"Joining a student initiative proved to be the best decision that I made last year.","tags":["Personal Insights","Student Initiative"],"title":"Working for Free Improved My Life in 3 Great Ways","type":"post"},{"authors":null,"categories":null,"content":"This article was first published on Medium.\nLet\u0026rsquo;s face it. You\u0026rsquo;re an aspiring Data Scientist. You need to desperately solve a classification task either to impress your professor or to finally become one of these mysterious Kaggle winners. The problem is that you\u0026rsquo;ve got no time. The presentation or the Kaggle competition is due in 24 hours. Of course, you could stick to a simple logit regression and call it a day. \u0026ldquo;No, not this time!\u0026rdquo; you think, and you\u0026rsquo;re browsing the web to find THE SOLUTION. But it better be fast.\nOkay, this is a highly improbable event. However, I wanted to share an easy XGBoost implementation that proved to be lightning-fast (compared to other solutions) that can help you to achieve a more stable and accurate model.\nHyperparameter-tuning was the step in which I lost most of my \u0026ldquo;valuable\u0026rdquo; time. I had to choose between different options ranging from manually trying out different hyperparameter combinations to more advanced methods like grid or random search. I felt overwhelmed. Ultimately, it took quite a while to find a technique that fulfilled my needs. Therefore, the focus will be on this step of building an XGBoost model.\nWhy are we using XGBoost again? Because it rocks. It is an optimized distributed gradient boosting library that has been used with great success on many machine learning challenges. Among the advantages of XGBoost are that its models are easily scalable, tend to avoid overfitting, and can be used for a wide range of problems [1].\nOf course, only preparing the data and executing XGBoost would somehow work. But clearly, there is no free lunch in data science. This procedure would leave you with a model that is most likely overfitted and therefore performs badly on an out-of-sample data set. Meaning: you have created a useless model that you should not use with other data.\nLuckily, XGBoost offers several ways to make sure that the performance of the model is optimized. Hyperparameter-tuning is the last part of the model building and can increase your model\u0026rsquo;s performance. Tuning is a systematic and automated process of varying parameters to find the \u0026ldquo;best\u0026rdquo; model.\nIn my opinion, learning is best done through a reproducible example, so please feel free to copy the code and run it in RStudio (or any other IDE) if you are experiencing any troubles with my explanations. I assume a basic understanding of machine learning, so please use Google if anything is not clear.\nIn the first part, I build (together with you) a basic XGBoost model. Thus, we are loading, preparing, and transforming the data, and at the end of this section, we will have received our first results. In the second part, I discuss my experiences with several hyperparameter-tuning methods. For our final model, we will use a random search algorithm to increase the predictive accuracy of our binary classification task.\nLet\u0026rsquo;s get started.\nBuilding an XGBoost model with (mostly) default parameters I chose to use the famous Titanic data set from Kaggle. If you have a Kaggle account, you can get the data here. If you don\u0026rsquo;t have a Kaggle account, you can also check out my GitHub repository, where you\u0026rsquo;ll find the data sets and the scripts of this blog post. As an aspiring data scientist, you most likely have at least one of these accounts.\nOur task is to predict whether a passenger survived the tragic titanic incident or not‚Ää-‚Ääa typical binary classification task.\nFirst, we load the necessary packages and train.csv and test.csv. Then we can have a look at the structure of the data.\n\r'data.frame': 891 obs. of 12 variables:\r$ PassengerId: int 1 2 3 4 5 6 7 8 9 10 ...\r$ Survived : int 0 1 1 1 0 0 0 0 1 1 ...\r$ Pclass : int 3 1 3 1 3 3 1 3 3 2 ...\r$ Name : chr \u0026quot;Braund, Mr. Owen Harris\u0026quot; \u0026quot;Cumings, Mrs. John Bradley (Florence Briggs Thayer)\u0026quot; \u0026quot;Heikkinen, Miss. Laina\u0026quot; \u0026quot;Futrelle, Mrs. Jacques Heath (Lily May Peel)\u0026quot; ...\r$ Sex : chr \u0026quot;male\u0026quot; \u0026quot;female\u0026quot; \u0026quot;female\u0026quot; \u0026quot;female\u0026quot; ...\r$ Age : num 22 38 26 35 35 NA 54 2 27 14 ...\r$ SibSp : int 1 1 0 1 0 0 0 3 0 1 ...\r$ Parch : int 0 0 0 0 0 0 0 1 2 0 ...\r$ Ticket : chr \u0026quot;A/5 21171\u0026quot; \u0026quot;PC 17599\u0026quot; \u0026quot;STON/O2. 3101282\u0026quot; \u0026quot;113803\u0026quot; ...\r$ Fare : num 7.25 71.28 7.92 53.1 8.05 ...\r$ Cabin : chr \u0026quot;\u0026quot; \u0026quot;C85\u0026quot; \u0026quot;\u0026quot; \u0026quot;C123\u0026quot; ...\r$ Embarked : chr \u0026quot;S\u0026quot; \u0026quot;C\u0026quot; \u0026quot;S\u0026quot; \u0026quot;S\u0026quot; ...\r\u0026gt; str(test)\r'data.frame': 418 obs. of 11 variables:\r$ PassengerId: int 892 893 894 895 896 897 898 899 900 901 ...\r$ Pclass : int 3 3 2 3 3 3 3 2 3 3 ...\r$ Name : chr \u0026quot;Kelly, Mr. James\u0026quot; \u0026quot;Wilkes, Mrs. James (Ellen Needs)\u0026quot; \u0026quot;Myles, Mr. Thomas Francis\u0026quot; \u0026quot;Wirz, Mr. Albert\u0026quot; ...\r$ Sex : chr \u0026quot;male\u0026quot; \u0026quot;female\u0026quot; \u0026quot;male\u0026quot; \u0026quot;male\u0026quot; ...\r$ Age : num 34.5 47 62 27 22 14 30 26 18 21 ...\r$ SibSp : int 0 1 0 0 1 0 0 1 0 2 ...\r$ Parch : int 0 0 0 0 1 0 0 1 0 0 ...\r$ Ticket : chr \u0026quot;330911\u0026quot; \u0026quot;363272\u0026quot; \u0026quot;240276\u0026quot; \u0026quot;315154\u0026quot; ...\r$ Fare : num 7.83 7 9.69 8.66 12.29 ...\r$ Cabin : chr \u0026quot;\u0026quot; \u0026quot;\u0026quot; \u0026quot;\u0026quot; \u0026quot;\u0026quot; ...\r$ Embarked : chr \u0026quot;Q\u0026quot; \u0026quot;S\u0026quot; \u0026quot;Q\u0026quot; \u0026quot;S\u0026quot; ...\r After getting an overview, we need to perform data preparation and feature engineering tasks. I just used some common sense and looked for some examples on Kaggle. Feature engineering is not the focus of this post. Therefore, I stick to the basics. But be aware that feature engineering is generally a crucial step and can significantly enhance the performance of your model. If you are not writing a blog post about hyperparameter-tuning, then spend much more time on this step! If you need individual guidance with excellent explanations, have a look here.\nTo perform these tasks, we bind the rows of the train and test set into one data frame. XGBoost needs to be able to work with the data. Therefore, we need to one-hot encode the data sets into sparse matrices. This is done because XGBoost only works with numeric or integer variables. Since we have factor variables in our data sets, the following approach creates one column per feature level. So instead of one row for Pclass, we will obtain the rows Pclass1, Pclass2, and Pclass3. This is also the reason why I put train and test set into one data set. Previously, when performing these tasks independently, the structure of the data got messed up.\n\rAfter conducting the data preparation and the feature engineering tasks, we split train and test set again.\nAfter these steps, the structure of the train and test data set is the same. Only the Survived variable is not present in the test set.\n'data.frame': 891 obs. of 13 variables:\r$ Pclass1 : num 0 1 0 1 0 0 1 0 0 0 ...\r$ Pclass2 : num 0 0 0 0 0 0 0 0 0 1 ...\r$ Pclass3 : num 1 0 1 0 1 1 0 1 1 0 ...\r$ Sexmale : num 1 0 0 0 1 1 1 1 0 0 ...\r$ Age : num 22 38 26 35 35 NA 54 2 27 14 ...\r$ SibSp : num 1 1 0 1 0 0 0 3 0 1 ...\r$ Parch : num 0 0 0 0 0 0 0 1 2 0 ...\r$ Fare : num 7.25 71.28 7.92 53.1 8.05 ...\r$ EmbarkedC: num 0 1 0 0 0 0 0 0 0 1 ...\r$ EmbarkedQ: num 0 0 0 0 0 1 0 0 0 0 ...\r$ EmbarkedS: num 1 0 1 1 1 0 1 1 1 0 ...\r$ Family : num 1 1 0 1 0 0 0 4 2 1 ...\r$ Survived : int 0 1 1 1 0 0 0 0 1 1 ...\r\u0026gt; str(test)\r'data.frame': 418 obs. of 12 variables:\r$ Pclass1 : num 0 0 0 0 0 0 0 0 0 0 ...\r$ Pclass2 : num 0 0 1 0 0 0 0 1 0 0 ...\r$ Pclass3 : num 1 1 0 1 1 1 1 0 1 1 ...\r$ Sexmale : num 1 0 1 1 0 1 0 1 0 1 ...\r$ Age : num 34.5 47 62 27 22 14 30 26 18 21 ...\r$ SibSp : num 0 1 0 0 1 0 0 1 0 2 ...\r$ Parch : num 0 0 0 0 1 0 0 1 0 0 ...\r$ Fare : num 7.83 7 9.69 8.66 12.29 ...\r$ EmbarkedC: num 0 0 0 0 0 0 0 0 1 0 ...\r$ EmbarkedQ: num 1 0 1 0 0 0 1 0 0 0 ...\r$ EmbarkedS: num 0 1 0 1 1 1 0 1 0 1 ...\r$ Family : num 0 1 0 0 2 0 0 2 0 2 ...\r Remember the overfitting problem that I talked about? To obtain a better model, this problem needs to be prevented while ensuring that the model learns the generalized underlying relationships of the features with the target variable. So we use the train set and split it into a training and a validation set. We use 80% of the passengers for training the model and 20% for the validation of the model. Keep on waiting. I will explain what the validation set does in a few paragraphs.\n\rAfterward, we will create xgb.DMatrices. These are optimized matrices for XGBoost. The label attribute specifies the target variable. We cannot specify a label for the test set since we do not have any information about it.\n\rFinally, we can train our first XGBoost model. We use the xgboost.train() command, and the gradient booster tree and the objective of binary classification are specified. The maximum number of iterations is set to 1,000. After these settings, the boosting algorithm could be implemented without further specifications on the training set. When keeping the default values, the algorithm would create 1,000 iterations of the model.\nWhat is the problem here? Imagine you are setting the number of iterations to a high number. Then the model iterates forever, and in the end, you would get an accuracy of 100% on your training data. When testing this model on the test set, something strange would happen. ‚ÄúWhy do I have only [put any low number here]% accuracy?! I thought my model was perfect!‚Äù. True, perfect in the sense that it perfectly describes the underlying data of your training set. Generalizable on other data? No.\nTo prevent this behavior, we create the validation set that I have talked about. We use the training set to train a model. But instead of training it until eternity, we set the early_stopping_rounds parameter to 50. Setting this parameter is important because it will stop the training of the model when the accuracy of the validation set has not improved for the specified number of rounds. When this is the case, then the algorithm automatically chooses the iteration with the highest accuracy. This setting prevents our overfitting problem.\nIn the following figure, you can quickly see this behavior. I display accuracy (1-error) and AUC of an XGBoost model depending on the iteration for the training and the validation set. Following my explanations, the training set would quickly reach an accuracy of 96% after 60 rounds. Luckily, our algorithm detects that the validation accuracy is not improving after ten iterations and therefore stops the algorithm at this iteration. Interestingly, the AUC is improving until round 60, so when changing the evaluation metric, we would get another result. Therefore, a change in the evaluation metric can lead to different results. Since we want to optimize the accuracy of the model, we stick to accuracy as an evaluation metric.\n\rAccuracy and AUC by Iteration of a XGBoost Model. Plot created by author.\r\rOkay, now it‚Äôs time to reveal the default XGBoost algorithm. Specifying the watchlist is an important step here because it is the parameter that tells XGBoost to stop iterating when the validation accuracy (1-error) does not improve anymore.\n\rMultiple eval metrics are present. Will use val_error for early stopping.\rWill train until val_error hasn't improved in 50 rounds.\r[11] train-auc:0.959340 train-error:0.099719 val-auc:0.856732 val-error:0.139665 [21] train-auc:0.970900 train-error:0.085674 val-auc:0.858648 val-error:0.134078 [31] train-auc:0.981661 train-error:0.070225 val-auc:0.860495 val-error:0.145251 [41] train-auc:0.988527 train-error:0.051966 val-auc:0.865969 val-error:0.145251 [51] train-auc:0.992277 train-error:0.042135 val-auc:0.868979 val-error:0.156425 Stopping. Best iteration:\r[10] train-auc:0.959241 train-error:0.101124 val-auc:0.853243 val-error:0.128492\r We then predict Survival of the validation set with our model. We display our results with the confusionMatrix command. Ahhhh important information: this is only the result of our validation set! Right now, we want to get a feeling of how good our model COULD be. Please do not confuse it with the test that we will perform afterward with the test set.\n\rPrediction Not Survived Survived\rNot Survived 109 16\rSurvived 7 47\rAccuracy : 0.8715 95% CI : (0.8135, 0.9168)\rNo Information Rate : 0.648 P-Value [Acc \u0026gt; NIR] : 1.189e-11\rKappa : 0.7088\rMcnemar's Test P-Value : 0.09529 Sensitivity : 0.7460 Specificity : 0.9397 Pos Pred Value : 0.8704 Neg Pred Value : 0.8720 Prevalence : 0.3520 Detection Rate : 0.2626 Detection Prevalence : 0.3017 Balanced Accuracy : 0.8428 'Positive' Class : Survived\r What comes first to my mind is that our first model is especially great in predicting people that died (as seen by the specificity metric). To make a statement about how great our first attempt is, we must assess the balance (in our case imbalance) of the target variable. How many people have survived? How many people died? Quick calculations:\nP(prediction=0) = P(class=0) = 0.648\rP(prediction=1) = P(class=1) = 0.352\racc = P(class=0) * P(prediction=0) + P(class=1) * P(prediction=1)\r= (0.648 * 0.648) + (0.352 * 0.352)\r= 0.5438\r So what we can say is that our model performed 32.77 percentage points (0.8715‚Äì0.5438) better than a ‚Äúweighted guess‚Äù (thanks to this post for refreshing my knowledge). But as I said, this is only the result of the validation set which we used to optimize our model. So let‚Äôs test our default model by submitting it to Kaggle.\nAfter uploading the predictions of the test set to Kaggle, we get our first result. We reached an accuracy of 76.07%. So we lost around 11% compared to the validation accuracy‚Ä¶DAMN.\n \u0026ldquo;But maybe there is light at the end of the tunnel?! What if hyperparameter-tuning is the solution?!\u0026rdquo;\n Hyperparameter-Tuning of an XGBoost Model There are different approaches to select hyperparameters. Due to this high degree of choice, many users choose the values of hyperparameters based on reputation, intuitive appeal, or adhere to the default parameter values. This may result in a model whose potential has not been fully utilized [2]. Okay, what have we learned? A structured hyperparameter-tuning process can increase the potential of our model!\nWhat I certainly did was to look at these great posts to get a feeling of how I could do it: here and here.\nSome authors used an iterative manual search approach. They selected hyperparameter-values, ran the model, looked at the output, found some logic, and repeated the whole process. Some first optimized the learning rate, other the number of tree leaves, etc. For me, I could not find one clear routine and it is definitely a time-consuming process. Also, it does not seem to follow the definition of tuning since this process is not really systematic and automated.\nWay more advanced, I found hyperparameter-tuning procedures in the mlr and caret packages that make use of grid and random search procedures. But what are grid and random search? The grid method runs all possible combinations of predefined values for hyperparameters. The higher the number of discrete values for hyperparameters and the more hyperparameters, the more computationally expensive this method. To be specific, grid search creates an exponentially increasing number of models the more values and the more hyperparameters are used. Thus, it gets difficult to handle quite quickly. With random search, one only defines the search space and sets the number of models that have to be created.\nSo I had a look into the literature and found a great paper from Bergstra et al. (2012). Their research shows that the random search approach has a higher efficiency compared to grid trials and manual search when granting the same computation time. Also, contradictory to the manual search of hyperparameters, the results acquired through random search are reproducible [3]. NICE!\nActually, I build a random search algorithm with the mlr package, but I faced severe problems. Due to the complex structure of this algorithm, the process had long running times and I could only test around 150 models per hour. Definitely too slow for an algorithm that takes less around 1 second to execute. Sure, k-fold cross-validation sounds really cool and provides more reliable results, but I just was not patient enough. Luckily, we have out-of-bag observations as provided in the test set through which we can evaluate the model performance of the trained algorithm. Also, testing my results became an issue. I could just not say with certainty that I created the algorithm with the right settings. This was due to the fact that I could not compare the results of the algorithm and a simple XGBoost model with the same hyperparameter-values because of k-fold cross-Validation.\nSo what did I do? In the end, I just created 10,000 random hyperparameter-value sets within a given search space with a for-loop. Then I executed the XGBoost algorithm 10,000 times with the predefined hyperparameter value sets. Of course, I saved the hyperparameter values and the corresponding validation accuracy as a csv-file. Specifying a seed before the random creation of the hyperparameters and the search algorithm ensured that the results are reproducible.\n\rTo evaluate my approach, I asked myself the following questions:\n  Do I understand my procedure? Yes.\n  Can I explain this approach to my professor or my fellow Kaggle competitors? Yes.\n  How long did it take to compare 10,000 XGBoost models with differing hyperparameter values? 29.7 minutes with a standard consumer laptop. Is this fast? Yes. Could I increase the speed with parallel computing if I wanted? Yes.\n  Can I state that I have found the best model? No. Although a high number of models are created, it is clear that the described approach has not found the very best model performance. To obtain better results, one could alter the search space or even include more hyperparameters. Still, finding the perfect model could not be stated after additional random searches with changed settings.\n  Does that procedure lead to the most stable model there is? Most likely not. As I said, using other methods (k-fold cross-validation, stratification, etc.) could lead to a more stable model.\n  So in the end, I decided on a procedure that is both fast and easy.\nLet‚Äôs have a look at the random search table.\n\r\rValues are rounded. If only copied like this into the model, you will obtain different results.\nIndeed, hyperparameter tuning has a strong effect on the performance of the model. The validation accuracy ranges between 80.4 percent and 89.4 percent, with a median of 86.6 percent and a mean of 86.7 percent. Remember, the validation accuracy that we got from an XGBoost model with default values was 87.2 percent‚Ä¶\nTo see more details, we plug the hyperparameters of the best hyperparameter value set into our XGBoost algorithm and again have a look at the model‚Äôs statistics and the confusion matrix.\n\rPrediction Not Survived Survived\rNot Survived 110 13\rSurvived 6 50\rAccuracy : 0.8939 95% CI : (0.8392, 0.9349)\rNo Information Rate : 0.648 P-Value [Acc \u0026gt; NIR] : 4.303e-14\rKappa : 0.7612\rMcnemar's Test P-Value : 0.1687 Sensitivity : 0.7937 Specificity : 0.9483 Pos Pred Value : 0.8929 Neg Pred Value : 0.8943 Prevalence : 0.3520 Detection Rate : 0.2793 Detection Prevalence : 0.3128 Balanced Accuracy : 0.8710 'Positive' Class : Survived\r Most significant improvement: sensitivity has increased by 4.77 percentage points. Having better validation results is great, but in the end, only our test accuracy score is essential. Now, the final moment has come‚Ä¶\n\rWe get an accuracy of 77.99%. This is better than the accuracy of our base model. But how have we done against other Kaggler‚Äôs? Hmmm‚Ä¶it doesn‚Äôt look too good for our model, as we can see in the below plot. We are only under the top 37%. That‚Äôs why we come to the final learning of my blog post. Hyperparameter-tuning is the last part of the model creation process. Since I got too excited about my search algorithm, I did not put enough time in feature engineering my model. Without useful features that the model can learn from, I can do all the hyperparameter-tuning I want. It will still be just mediocre. Therefore, when time is limited, one should focus on feature engineering and not on hyperparameter-tuning.\n\rHistogram of the public leaderboard scores. The models that have had better accuracy than our model are displayed in black, the others are displayed in grey. We would have gotten an accuracy of 0.627 if we had predicted that everyone died. An accuracy of 0.766 would have been achieved if we had predicted that all males over the age of three died and that the women and children survived. Plot created by author.\r\rWere all the efforts for nothing? Of course not. If you have enough time, you can create great features and use a hyperparameter-tuning process (in the end!) that helps you to extract the very last accuracy percentage points out of your model.\nConclusion What I have introduced here is a procedure that I used to tune the model that I created for a seminar paper. Although plenty of information can be found via Google, I struggled to find a suitable solution for my needs. What I wanted, in the end, was something that I could correctly understand and which did not require endless hours to compute. Also, I wanted an automated approach whose results I could replicate at any point in time. These criteria are definitely fulfilled by my simple random search algorithm.\nI don‚Äôt claim that this procedure leads to the best or the most stable model. If you had the pleasure to stumble upon a better solution, have any questions or comments, feel free to reach me via the contact field or LinkedIn.\nReferences [1] Chen, T. \u0026amp; Guestrin, C. (2016), Xgboost: A scalable tree boosting system, in B. Krishnapuram \u0026amp; M. Shah, eds, ‚ÄòKDD ‚Äô16: Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining‚Äô, pp. 785‚Äì794.\n[2] Thornton, C., Hutter, F., Hoos, H. H. \u0026amp; Leyton-Brown, K. (2013), Autoweka: Combined selection and hyperparameter optimization of classification algorithms, in R. Ghani, T. E. Senator, P. Bradley, R. Parekh \u0026amp; J. He, eds, ‚ÄòKDD ‚Äô13: Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining‚Äô, pp. 847‚Äì855.\n[3] Bergstra, J. \u0026amp; Bengio, Y. (2012), ‚ÄòRandom search for hyper-parameter optimization‚Äô, Journal of Machine Learning Research 12, 281‚Äì305.\n","date":1590105600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1590105600,"objectID":"e5972d97568ada12e6c8494737a15276","permalink":"/post/getting-to-a-hyperparameter-tuned-xgboost-model-in-no-time/","publishdate":"2020-05-22T00:00:00Z","relpermalink":"/post/getting-to-a-hyperparameter-tuned-xgboost-model-in-no-time/","section":"post","summary":"How to fine-tune your first XGBoost model in R with random search.","tags":["Data Science"],"title":"Getting to a Hyperparameter-Tuned XGBoost Model in No Time","type":"post"},{"authors":null,"categories":["Data Science","Research"],"content":"About the paper This research paper was written in summer 2020 within the Master\u0026rsquo;s seminar Data Mining in Marketing: Data Driven Customer Analytics with Machine Learning. In partial fulfillment of the requirements of the seminar, I predicted and interpreted cross-selling purchase probabilities using XGBoost and SHAP values in R. Read the full paper here: Boosting Gradient Boosting Interpretability: Predicting and Interpreting Cross-Selling Purchase Probabilities of a Large German Savings Bank.\nIntroduction and Findings In this paper, a dataset from a large German savings bank is used to predict crossselling purchase probabilities and decisions in the customer base. The goals of this paper are (1) to accurately predict whether an already existing customer will open a checking account and (2) to explore which effect the features have on the prediction to enhance the interpretability of the model. To reach these goals, the paper leverages one of the leading gradient boosting algorithms there is, namely XGBoost. It has been used with great success on many machine learning and data mining challenges. Among the advantages of XGBoost are that its models are easily scalable, tend to avoid overfitting, and can be used for a wide range of problems (Chen \u0026amp; Guestrin 2016, p. 785-786). To tackle the lack of easy interpretability of boosted trees (Friedman 2001, p. 1229‚Äì1230), this paper implements SHapley Additive exPlanations (SHAP) values to explain the output of the XGBoost model.\nWhen accuracy is the main goals, then one should use more complex models. Although tuning an XGBoost model is computationally expensive, once it is tuned, it becomes computationally cheap. It becomes apparent that such prediction systems have their place in marketing analytics departments. Instead of only predicting the likelihood of buying a checking account, marketing departments could extend this approach to any product offering to compare purchasing probabilities and target customers with the products for which they have the highest probability to buy. Through this strategy, it could become feasible to target the right customers and to ultimately increase profitability. For this concept to be successful, several additional topics need to be addressed. One has to assess which customers have the potential to be profitable while excluding customers that lead to losses (Shah et al. 2012). Also, one must answer the question when and where a customer should be targeted.\nGenerally, SHAP values enable its users to critically examine complex models and to understand how dependent variables were predicted. Through this method, users gain further knowledge about importance, extent and direction of feature variables on the target variable. Although causal statements cannot be made through this approach, it still helps users to gain trust in the model, to find ways of improving the model, and to get a new understanding of the data. Therefore, this enhanced interpretability should increase user adoption. When only using tools from the xgboost package, this would not be feasible to such an extent. The trust of the analysis through SHAP values is increased because suggested underlying trends of the data have been amongst others discovered by RFM-models (Bauer 1988, Miglautsch 2000). Customers that have recently acquired another product have an higher prediction value than customers that have not bought another product for a longer period of time. Also, the more active customers are (as measured by logins), the higher is the prediction that these customers cross-buy a checking account. Other important trends found in the data are that younger customers exhibit an higher prediction probability than older customers and that checking account ads always lead to a positive effect on the prediction, although with varying extent. This paper shows that giro_mailing does lead to a negative interaction effect on the prediction when appearing with younger or recent customers. This could indicate that the model finds autoresponse within the group of younger or recent customers and punishes this effect with a negative interaction value.\nXGBoost models should be used in practice for predicting cross-selling purchase probabilities and decisions. One of the biggest disadvantages - lack of interpretability - can be mitigated through the use of SHAP values that greatly expand the transparency, explainability, interpretability of complex tree-based models.\n","date":1580083200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1580083200,"objectID":"13348f02b61a1a4918c3b660ce523ae5","permalink":"/project/boosting-gradient-boosting-interpretability/","publishdate":"2020-01-27T00:00:00Z","relpermalink":"/project/boosting-gradient-boosting-interpretability/","section":"project","summary":"I predicted and interpreted cross-selling purchase probabilities using XGBoost and SHAP values in R.","tags":["Research","Data Science"],"title":"Boosting Gradient Boosting Interpretability","type":"project"},{"authors":null,"categories":["Research"],"content":"About the paper This research paper was written in winter 2019/20 within the Master\u0026rsquo;s seminar Big Data in Personal Finance. In partial fulfillment of the requirements of the lecture, I analyzed the Survey of Consumer Finances 2016 with regards to the effect of financial literacy to stock market participation using R. Read the full paper here: Financial Literacy and Stock Market Participation: Evidence From the Survey of Consumer Finances.\nIntroduction and Findings The stockholding puzzle, pioneered in the research of Haliassos and Bertaut (1995), has gained a lot of attention by researchers. The question is why a large part of the population worldwide does not invest in stocks despite the equity premium and prefers to hold their assets in low-rate liquid assets (Haliassos \u0026amp; Bertaut, 1995, p. 1110). Due to the demographic shift in the population, this question has become more important in the last years. Stocks and their long-term wealth generating potential can help retirees to not be dependent on the struggling social security and pension systems (Christelis, Georgarakos, \u0026amp; Haliassos, 2011, p. 1918).\nThe contribution to the existing literature is that this paper analyzes the Survey of Consumer Finances (SCF) 2016 with regards to the effect of financial literacy to stock market participation. A new set of financial literacy questions was included in the SCF 2016 that makes it feasible to get an objective measure of financial literacy. This was not possible in earlier series of the SCF which resulted in financial literacy either not being included in the analysis [see, e.g. (Campell, 2006); (Haliassos \u0026amp; Bertaut, 1995); (Malmendier \u0026amp; Nagel, 2011)] or being included through proxies [see, e.g. (Christelis et al., 2011); (Huston et al., 2012); (Shum \u0026amp; Faig, 2006)]. While controlling for other effects, the measure of financial literacy used in this paper enables a more precise estimation of its effect on stock market participation.\nIn the descriptive analysis, I was able to show that there is a difference of stock market participation conditional on financial literacy. Also, financial literacy proxies used in previous research like education and wealth show structural differences in stock market participation when separated in financial literacy groups. More importantly, the Probit regression analysis confirms the results that financial literacy has indeed a significant effect in the decision to participate in the stock market. Therefore, I show that a lack of financial knowledge is a significant deterrent to stock ownership.\n","date":1580083200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1580083200,"objectID":"a0e96dceeae0c33c51306490885990bb","permalink":"/project/financial-literacy/","publishdate":"2020-01-27T00:00:00Z","relpermalink":"/project/financial-literacy/","section":"project","summary":"I analyzed the Survey of Consumer Finances 2016 with regards to the effect of financial literacy to stock market participation using R.","tags":["Research"],"title":"Financial Literacy and Stock Market Participation","type":"project"},{"authors":["Jonathan Ratschat"],"categories":null,"content":" Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including code and math.\n","date":1554595200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554595200,"objectID":"557dc08fd4b672a0c08e0a8cf0c9ff7d","permalink":"/publication/preprint/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/preprint/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Source Themes"],"title":"An example preprint / working paper","type":"publication"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Academic  Academic | Documentation\n Features  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides   Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click  PDF Export: E   Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)   Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\n Fragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  Two  Three \n A fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears   Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view    Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links    night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links   Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/img/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}   Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }   Questions?  Ask\n Documentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Academic's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":null,"categories":["Student Initiative"],"content":"","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"9e28a33e3b5b6cf842be5329f922029c","permalink":"/project/techacademy/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/techacademy/","section":"project","summary":"TechAcademy e.V. prepares students for the digital future in the fields of Data Science and Web Development. Have a look on our website.","tags":["Student Initiative"],"title":"TechAcademy","type":"project"},{"authors":["Jonathan Ratschat","Robert Ford"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including code and math.\n","date":1441065600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1441065600,"objectID":"966884cc0d8ac9e31fab966c4534e973","permalink":"/publication/journal-article/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/journal-article/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Source Themes"],"title":"An example journal article","type":"publication"},{"authors":["Jonathan Ratschat","Robert Ford"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including code and math.\n","date":1372636800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1372636800,"objectID":"69425fb10d4db090cfbd46854715582c","permalink":"/publication/conference-paper/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/conference-paper/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Source Themes"],"title":"An example conference paper","type":"publication"}]