<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Data Science | Jonathan Ratschat</title>
    <link>/category/data-science/</link>
      <atom:link href="/category/data-science/index.xml" rel="self" type="application/rss+xml" />
    <description>Data Science</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>Â© Jonathan Ratschat 2020</copyright><lastBuildDate>Sat, 26 Sep 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/img/icon.png</url>
      <title>Data Science</title>
      <link>/category/data-science/</link>
    </image>
    
    <item>
      <title>U.S. News</title>
      <link>/project/us-news/</link>
      <pubDate>Sat, 26 Sep 2020 00:00:00 +0000</pubDate>
      <guid>/project/us-news/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Boosting Gradient Boosting Interpretability</title>
      <link>/project/boosting-gradient-boosting-interpretability/</link>
      <pubDate>Mon, 27 Jan 2020 00:00:00 +0000</pubDate>
      <guid>/project/boosting-gradient-boosting-interpretability/</guid>
      <description>&lt;h2 id=&#34;about-the-paper&#34;&gt;About the paper&lt;/h2&gt;
&lt;p&gt;This research paper was written in summer 2020 within the Master&amp;rsquo;s seminar &lt;em&gt;Data Mining in Marketing: Data Driven Customer Analytics with Machine Learning&lt;/em&gt;. In partial fulfillment of the requirements of the seminar, I predicted and interpreted cross-selling purchase probabilities using XGBoost and SHAP values in R. Read the full paper here: 
&lt;a href=&#34;https://github.com/JRatschat/Boosting-Gradient-Boosting-Interpretability/blob/master/Boosting_Gradient_Boosting_Interpretability.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Boosting Gradient Boosting Interpretability: Predicting and Interpreting Cross-Selling Purchase Probabilities of a Large German Savings Bank&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;introduction-and-findings&#34;&gt;Introduction and Findings&lt;/h2&gt;
&lt;p&gt;Powerful analytical methods lead to more efficient and effective data-driven marketing (Wedel &amp;amp; Kannan 2016). Especially machine learning has become popular in this field due to the high predictiveness of its algorithms. A challenge is, however, that complex machine learning models are generally black-box models. Hence, data goes in, and results come out, but it is unknown or hidden to its users how these models came up with its results. Increasing interpretability is vital because it grows users&amp;rsquo; confidence and trust in machine learning models. Users do not adopt models that fail to do so (Ribeiro et al. 2016). Also, enhanced interpretability extends the knowledge derived from models. Therefore, eliminating the tradeoff between a model&amp;rsquo;s accuracy and a model&amp;rsquo;s interpretability has gained many researchers&amp;rsquo; attention (Ribeiro et al. 2016, Lundberg &amp;amp; Lee 2017, Chen et al. 2018, Lipton 2018).&lt;/p&gt;
&lt;p&gt;In this paper, I use a data set from a large German savings bank to predict cross-selling purchase probabilities and decisions in the customer base. This paper aims to (1) to accurately predict whether an already existing customer will open a checking account and (2) to explore which effect the features have on the prediction to enhance the interpretability of the model. The paper leverages one of the leading gradient boosting algorithms, namely XGBoost, to reach these goals. It has been used with great success in many machine learning and data mining challenges (Chen &amp;amp; Guestrin 2016). Moreover, I implement SHapley Additive exPlanations (SHAP) values to tackle the lack of boosted trees&amp;rsquo; interpretability (Friedman 2001).&lt;/p&gt;
&lt;p&gt;Regarding the first research question, a hyperparameter-tuned XGBoost model&amp;rsquo;s predictive accuracy proves superior compared to a benchmark logit model. A disadvantage, however, is that more complex models are more computationally expensive than simple models. Concerning the second research question, it becomes clear that SHAP values enable its users to critically examine complex models and understand how dependent variables were predicted. Through this method, users gain further knowledge about the importance, extent, and direction of feature variables on the target variable. Although causal statements cannot be made through this approach, it still helps users gain trust in the model, find ways to improve the model and get a new understanding of the data. When using ordinary feature impact tools, this would not be feasible to such an extent.&lt;/p&gt;
&lt;p&gt;The analysis unfolds that this so-called black-box model applies among other trends discovered in the research of RFM-models (Bauer 1988, Miglautsch 2000). For example, customers that have recently acquired another product have a higher predictive value of opening a checking account than customers who have not. Also, the more active customers are (as measured by logins), the higher is the prediction value. Other trends found in the data are that younger customers exhibit a higher prediction probability than older customers and that checking account ads always lead to a positive effect on the prediction, although varying.&lt;/p&gt;
&lt;p&gt;The paper&amp;rsquo;s main conclusion is that XGBoost models have their place in practice for predicting cross-selling purchase probabilities and decisions. One of the most significant disadvantages - lack of interpretability - can be mitigated with SHAP values that greatly expand the transparency, explainability, interpretability of complex tree-based models.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
